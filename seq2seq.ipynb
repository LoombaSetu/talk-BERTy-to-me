{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Change working directory to talk-berty-to-me root\n",
    "import os\n",
    "os.chdir(\"D:/University/Projects/AML/talk-berty-to-me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_gutenberg = pd.read_csv('data/books_and_genres.csv')\n",
    "# dataset_train = data_gutenberg.sample(frac=0.6, random_state=0)\n",
    "# dataset_val = data_gutenberg.drop(dataset_train.index).sample(frac=0.5, random_state=0)\n",
    "# dataset_test = data_gutenberg.drop(dataset_train.index).drop(\n",
    "#     dataset_val.index)\n",
    "# dataset_train.to_parquet('data/datasets/train.parquet', index=False)\n",
    "# dataset_test.to_parquet('data/datasets/test.parquet', index=False)\n",
    "# dataset_val.to_parquet('data/datasets/val.parquet', index=False)\n",
    "# data_gutenberg.sample(20).to_parquet('data/datasets/dev.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add code to switch between datasets here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = pd.read_parquet('data/datasets/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_accents(input_str):\n",
    "    decomposed = unicodedata.normalize('NFD', input_str)\n",
    "    return not all([unicodedata.category(char) != 'Mn' for char in decomposed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/books_and_genres_eng.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[:,'is_accent'] = raw_data.loc[:,'text'].apply(contains_accents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.loc[raw_data.loc[:,'is_accent'] == False, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.drop_duplicates(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "      <th>is_accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>the power and the glory</td>\n",
       "      <td>Produced by Juliet Sutherland, Sjaani and PG D...</td>\n",
       "      <td>{'literary-fiction', 'christian', 'history', '...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>paradise</td>\n",
       "      <td>Produced by Judith Smith and Natalie Salter\\n\\...</td>\n",
       "      <td>{'literary-fiction', 'mythology', 'historical-...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147</td>\n",
       "      <td>persuasion</td>\n",
       "      <td>Produced by Sharon Partridge and Martin Ward. ...</td>\n",
       "      <td>{'romance', 'literary-fiction', 'classics', 'h...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>157</td>\n",
       "      <td>alcestis</td>\n",
       "      <td>Produced by Ted Garvin, Charles M. Bidwell and...</td>\n",
       "      <td>{'romance', 'literary-fiction', 'history', 'cl...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>292</td>\n",
       "      <td>dead souls</td>\n",
       "      <td>Produced by John Bickers\\n\\n\\n\\n\\n\\nDEAD SOULS...</td>\n",
       "      <td>{'literary-fiction', 'roman', 'classics', 'nov...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    title  \\\n",
       "0          31  the power and the glory   \n",
       "1          32                 paradise   \n",
       "4         147               persuasion   \n",
       "7         157                 alcestis   \n",
       "8         292               dead souls   \n",
       "\n",
       "                                                text  \\\n",
       "0  Produced by Juliet Sutherland, Sjaani and PG D...   \n",
       "1  Produced by Judith Smith and Natalie Salter\\n\\...   \n",
       "4  Produced by Sharon Partridge and Martin Ward. ...   \n",
       "7  Produced by Ted Garvin, Charles M. Bidwell and...   \n",
       "8  Produced by John Bickers\\n\\n\\n\\n\\n\\nDEAD SOULS...   \n",
       "\n",
       "                                              genres language_code  is_accent  \n",
       "0  {'literary-fiction', 'christian', 'history', '...           eng      False  \n",
       "1  {'literary-fiction', 'mythology', 'historical-...           eng      False  \n",
       "4  {'romance', 'literary-fiction', 'classics', 'h...           eng      False  \n",
       "7  {'romance', 'literary-fiction', 'history', 'cl...           eng      False  \n",
       "8  {'literary-fiction', 'roman', 'classics', 'nov...           eng      False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = raw_data.sample(20)\n",
    "dev_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "###Creating a validation set\n",
    "val_data = raw_data.sample(4)\n",
    "val_data.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "      <th>is_accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>no name</td>\n",
       "      <td>Produced by James Rusk and David Widger\\n\\n\\n\\...</td>\n",
       "      <td>{'romance', 'literary-fiction', 'crime', 'myst...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>the scarlet pimpernel</td>\n",
       "      <td>Produced by Conway Yee and David Widger\\n\\n\\n\\...</td>\n",
       "      <td>{'romance', 'adventure', 'literary-fiction', '...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>creatures of the night</td>\n",
       "      <td>Produced by David Edwards, Marcia Brooks and t...</td>\n",
       "      <td>{'horror', 'animals', 'comics', 'fantasy', 'pa...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>the frogs</td>\n",
       "      <td>Produced by Ted Garvin, Marvin A. Hodges, Char...</td>\n",
       "      <td>{'classics', 'mythology', 'literature', 'ficti...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>after dark</td>\n",
       "      <td>Produced by James Rusk\\n\\n\\n\\n\\n\\nAFTER DARK\\n...</td>\n",
       "      <td>{'contemporary', 'literary-fiction', '21st-cen...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "97                  no name   \n",
       "366   the scarlet pimpernel   \n",
       "303  creatures of the night   \n",
       "390               the frogs   \n",
       "139              after dark   \n",
       "\n",
       "                                                  text  \\\n",
       "97   Produced by James Rusk and David Widger\\n\\n\\n\\...   \n",
       "366  Produced by Conway Yee and David Widger\\n\\n\\n\\...   \n",
       "303  Produced by David Edwards, Marcia Brooks and t...   \n",
       "390  Produced by Ted Garvin, Marvin A. Hodges, Char...   \n",
       "139  Produced by James Rusk\\n\\n\\n\\n\\n\\nAFTER DARK\\n...   \n",
       "\n",
       "                                                genres language_code  \\\n",
       "97   {'romance', 'literary-fiction', 'crime', 'myst...           eng   \n",
       "366  {'romance', 'adventure', 'literary-fiction', '...         en-US   \n",
       "303  {'horror', 'animals', 'comics', 'fantasy', 'pa...           eng   \n",
       "390  {'classics', 'mythology', 'literature', 'ficti...         en-US   \n",
       "139  {'contemporary', 'literary-fiction', '21st-cen...           eng   \n",
       "\n",
       "     is_accent  \n",
       "97       False  \n",
       "366      False  \n",
       "303      False  \n",
       "390      False  \n",
       "139      False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "      <th>is_accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>the frogs</td>\n",
       "      <td>Produced by Ted Garvin, Marvin A. Hodges, Char...</td>\n",
       "      <td>{'classics', 'mythology', 'literature', 'ficti...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>eastern standard tribe</td>\n",
       "      <td>Eastern Standard Tribe\\n\\nCory Doctorow\\n\\nCop...</td>\n",
       "      <td>{'literary-fiction', 'science-fiction', 'fanta...</td>\n",
       "      <td>en-US</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>the tale of the flopsy bunnies</td>\n",
       "      <td>Produced by Michael Ciesielski and the Online ...</td>\n",
       "      <td>{'picture-books', 'animals', 'classics', 'fant...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>the untouchable</td>\n",
       "      <td>Produced by Greg Weeks, David Wilson and the O...</td>\n",
       "      <td>{'literary-fiction', 'history', 'novella', 'hi...</td>\n",
       "      <td>eng</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "390                       the frogs   \n",
       "148          eastern standard tribe   \n",
       "92   the tale of the flopsy bunnies   \n",
       "220                 the untouchable   \n",
       "\n",
       "                                                  text  \\\n",
       "390  Produced by Ted Garvin, Marvin A. Hodges, Char...   \n",
       "148  Eastern Standard Tribe\\n\\nCory Doctorow\\n\\nCop...   \n",
       "92   Produced by Michael Ciesielski and the Online ...   \n",
       "220  Produced by Greg Weeks, David Wilson and the O...   \n",
       "\n",
       "                                                genres language_code  \\\n",
       "390  {'classics', 'mythology', 'literature', 'ficti...         en-US   \n",
       "148  {'literary-fiction', 'science-fiction', 'fanta...         en-US   \n",
       "92   {'picture-books', 'animals', 'classics', 'fant...           eng   \n",
       "220  {'literary-fiction', 'history', 'novella', 'hi...           eng   \n",
       "\n",
       "     is_accent  \n",
       "390      False  \n",
       "148      False  \n",
       "92       False  \n",
       "220      False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\setul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\setul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Function to clean text of books. Removes email addresses, new lines, html tags, and extra spaces.\n",
    "\n",
    "    Input: Text (String)\n",
    "    Output: Cleaned Text (String)\n",
    "    '''\n",
    "    cleaned_text = text.lower()\n",
    "    cleaned_text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' ', text)\n",
    "    cleaned_text = re.sub(r'^.*?(?=\\n\\n\\n)', ' ', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"[^>]*>.*?</a>', ' ', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s.?!]', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r' +', ' ', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_first_row(group):\n",
    "    return group.iloc[1:]\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['cleaned_text'] = dev_data['text'].apply(lambda x:clean_text(x))\n",
    "dev_data.loc[:,'genres'] = dev_data.loc[:,'genres'].apply(lambda x:clean_text(x))\n",
    "dev_data.loc[:,'sentences'] = dev_data.loc[:,'cleaned_text'].apply(lambda x: nltk.tokenize.sent_tokenize(str(x)))\n",
    "dev_data = dev_data.explode('sentences')\n",
    "dev_data.loc[:,'sentences'] = dev_data.loc[:,'sentences'].apply(lambda x:lowercase(x))\n",
    "dev_data.reset_index(drop=True, inplace=True)\n",
    "dev_data['label_sentences'] = dev_data.groupby('title')['sentences'].shift(-1)\n",
    "dev_data = dev_data.dropna(subset=['label_sentences'])\n",
    "dev_data = dev_data.loc[:,['title', 'genres','sentences', 'label_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['cleaned_text'] = val_data['text'].apply(lambda x:clean_text(x))\n",
    "val_data.loc[:,'genres'] = val_data.loc[:,'genres'].apply(lambda x:clean_text(x))\n",
    "val_data.loc[:,'sentences'] = val_data.loc[:,'cleaned_text'].apply(lambda x: nltk.tokenize.sent_tokenize(str(x)))\n",
    "val_data = val_data.explode('sentences')\n",
    "val_data.loc[:,'sentences'] = val_data.loc[:,'sentences'].apply(lambda x:lowercase(x))\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "val_data['label_sentences'] = val_data.groupby('title')['sentences'].shift(-1)\n",
    "val_data = val_data.dropna(subset=['label_sentences'])\n",
    "val_data = val_data.loc[:,['title', 'genres','sentences', 'label_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>no name by wilkie collins editorial note ital...</td>\n",
       "      <td>the main purpose of this story is to appeal to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>the main purpose of this story is to appeal to...</td>\n",
       "      <td>here is one more book that depicts the struggl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>here is one more book that depicts the struggl...</td>\n",
       "      <td>it has been my aim to make the character of ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>it has been my aim to make the character of ma...</td>\n",
       "      <td>this design was no easy one to accomplish and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>this design was no easy one to accomplish and ...</td>\n",
       "      <td>round the central figure in the narrative othe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     title                                             genres  \\\n",
       "0  no name   romance literary fiction crime mystery classi...   \n",
       "1  no name   romance literary fiction crime mystery classi...   \n",
       "2  no name   romance literary fiction crime mystery classi...   \n",
       "3  no name   romance literary fiction crime mystery classi...   \n",
       "4  no name   romance literary fiction crime mystery classi...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0   no name by wilkie collins editorial note ital...   \n",
       "1  the main purpose of this story is to appeal to...   \n",
       "2  here is one more book that depicts the struggl...   \n",
       "3  it has been my aim to make the character of ma...   \n",
       "4  this design was no easy one to accomplish and ...   \n",
       "\n",
       "                                     label_sentences  \n",
       "0  the main purpose of this story is to appeal to...  \n",
       "1  here is one more book that depicts the struggl...  \n",
       "2  it has been my aim to make the character of ma...  \n",
       "3  this design was no easy one to accomplish and ...  \n",
       "4  round the central figure in the narrative othe...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = dev_data.loc[:,['title', 'genres','sentences', 'label_sentences']]\n",
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the frogs</td>\n",
       "      <td>classics mythology literature fiction comedy ...</td>\n",
       "      <td>the harvard classics edited by charles w elio...</td>\n",
       "      <td>he is credited with over forty plays eleven of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the frogs</td>\n",
       "      <td>classics mythology literature fiction comedy ...</td>\n",
       "      <td>he is credited with over forty plays eleven of...</td>\n",
       "      <td>his satire deal with political religious and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the frogs</td>\n",
       "      <td>classics mythology literature fiction comedy ...</td>\n",
       "      <td>his satire deal with political religious and l...</td>\n",
       "      <td>the attic comedy was produced at the festivals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the frogs</td>\n",
       "      <td>classics mythology literature fiction comedy ...</td>\n",
       "      <td>the attic comedy was produced at the festivals...</td>\n",
       "      <td>aristophanes seems indeed to have been regarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the frogs</td>\n",
       "      <td>classics mythology literature fiction comedy ...</td>\n",
       "      <td>aristophanes seems indeed to have been regarde...</td>\n",
       "      <td>he died shortly after the production of his pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title                                             genres  \\\n",
       "0  the frogs   classics mythology literature fiction comedy ...   \n",
       "1  the frogs   classics mythology literature fiction comedy ...   \n",
       "2  the frogs   classics mythology literature fiction comedy ...   \n",
       "3  the frogs   classics mythology literature fiction comedy ...   \n",
       "4  the frogs   classics mythology literature fiction comedy ...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0   the harvard classics edited by charles w elio...   \n",
       "1  he is credited with over forty plays eleven of...   \n",
       "2  his satire deal with political religious and l...   \n",
       "3  the attic comedy was produced at the festivals...   \n",
       "4  aristophanes seems indeed to have been regarde...   \n",
       "\n",
       "                                     label_sentences  \n",
       "0  he is credited with over forty plays eleven of...  \n",
       "1  his satire deal with political religious and l...  \n",
       "2  the attic comedy was produced at the festivals...  \n",
       "3  aristophanes seems indeed to have been regarde...  \n",
       "4  he died shortly after the production of his pl...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = val_data.loc[:,['title', 'genres','sentences', 'label_sentences']]\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'now norah!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.loc[1905, 'sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Building vocabulary original and functional\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# tokenizer = get_tokenizer('basic_english')\n",
    "# vocab_iter = iter(dev_data.loc[:,'sentences'] + dev_data.loc[:,'title'] + dev_data.loc[:,'genres'])\n",
    "# def yield_tokens(train_iter):\n",
    "#     for text in train_iter:\n",
    "#         if not isinstance(text, str):\n",
    "#             if type(text) == list:\n",
    "#                 for t in text:\n",
    "#                     yield tokenizer(t)\n",
    "#             continue\n",
    "#         yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocabulary fixed\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab_iter = iter(dev_data.loc[:,'sentences'])\n",
    "def yield_tokens(train_iter):\n",
    "    for text in train_iter:\n",
    "        if not isinstance(text, str):\n",
    "            if type(text) == list:\n",
    "                for t in text:\n",
    "                    yield tokenizer(t)\n",
    "            continue\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(vocab_iter), specials=[\"<unk>\", \"<pad>\", \"<BOS>\", \"<EOS>\"], min_freq=25)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6276"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_CACHE_DIR = '/Users/setul/mlpp23/.vector_cache'\n",
    "glove = torchtext.vocab.GloVe('6B', cache=VECTOR_CACHE_DIR)\n",
    "glove_vectors = glove.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>if the aspect of mr. vanstone s character whic...</td>\n",
       "      <td>his next words raised the veil and showed the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>his next words raised the veil and showed the ...</td>\n",
       "      <td>chapter xiii.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>chapter xiii.</td>\n",
       "      <td>the fortune which mr. vanstone possessed when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>the fortune which mr. vanstone possessed when ...</td>\n",
       "      <td>mr. vanstone the elder was a manufacturer in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>mr. vanstone the elder was a manufacturer in t...</td>\n",
       "      <td>he married early in life and the children of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>no name</td>\n",
       "      <td>romance literary fiction crime mystery classi...</td>\n",
       "      <td>he married early in life and the children of t...</td>\n",
       "      <td>first michael the eldest son still living and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                                             genres  \\\n",
       "2480  no name   romance literary fiction crime mystery classi...   \n",
       "2481  no name   romance literary fiction crime mystery classi...   \n",
       "2482  no name   romance literary fiction crime mystery classi...   \n",
       "2483  no name   romance literary fiction crime mystery classi...   \n",
       "2484  no name   romance literary fiction crime mystery classi...   \n",
       "2485  no name   romance literary fiction crime mystery classi...   \n",
       "\n",
       "                                              sentences  \\\n",
       "2480  if the aspect of mr. vanstone s character whic...   \n",
       "2481  his next words raised the veil and showed the ...   \n",
       "2482                                      chapter xiii.   \n",
       "2483  the fortune which mr. vanstone possessed when ...   \n",
       "2484  mr. vanstone the elder was a manufacturer in t...   \n",
       "2485  he married early in life and the children of t...   \n",
       "\n",
       "                                        label_sentences  \n",
       "2480  his next words raised the veil and showed the ...  \n",
       "2481                                      chapter xiii.  \n",
       "2482  the fortune which mr. vanstone possessed when ...  \n",
       "2483  mr. vanstone the elder was a manufacturer in t...  \n",
       "2484  he married early in life and the children of t...  \n",
       "2485  first michael the eldest son still living and ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.iloc[2480:2486]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "if IS_CUDA:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_batch(batch):\n",
    "    titles, genres, sentences, label_sentences = zip(*batch)\n",
    "    #Add a separator tag between title and genre\n",
    "    context = [tokenizer(g) + ['<pad>'] + tokenizer(t) + ['<pad>'] +\n",
    "               ['<BOS>'] + tokenizer(s) + ['<EOS>'] for t, g,\n",
    "                s in zip(titles, genres, sentences)]\n",
    "    label_sentence = [['<BOS>'] + tokenizer(s) + ['<EOS>'] for s in label_sentences]\n",
    "    label_tensor = pad_sequence([torch.tensor(vocab.lookup_indices(t)) for t in label_sentence],\n",
    "                                    padding_value=vocab['<pad>'], batch_first=True)\n",
    "    encoder_tensor = pad_sequence([torch.tensor(vocab.lookup_indices(t)) for t in context],\n",
    "                                 padding_value=vocab['<pad>'], batch_first=True)\n",
    "    return encoder_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From HW4\n",
    "from torch.utils.data import Sampler\n",
    "class BatchSequentialSampler(Sampler):\n",
    "    r\"\"\"Samples batches, s.t. the ith elements of each batch are sequential.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        num_batches = len(self.data_source)//self.batch_size\n",
    "        for i in range(num_batches):\n",
    "            for j in range(self.batch_size):\n",
    "                yield(j * num_batches + i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_source)//self.batch_size) * self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = BatchSequentialSampler(dev_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataloader = torch.utils.data.DataLoader(dev_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']].values,\n",
    "                                                   batch_size=8, collate_fn=collate_batch, sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = BatchSequentialSampler(val_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(val_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']].values,\n",
    "                                                   batch_size=8, collate_fn=collate_batch, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 83])\n",
      "torch.Size([8, 107])\n"
     ]
    }
   ],
   "source": [
    "for idx, (context_tensor, label_tensor) in enumerate(batch_dataloader):\n",
    "    print(context_tensor.shape)\n",
    "    print(label_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN_encoder(nn.Module):\n",
    "    def __init__ (self, embedding_dim, hidden_dim,\n",
    "                  vocab_size, num_layers=2, type_rnn = 'LSTM', bidirectional = True,\n",
    "                  dropout = 0.3, pad_idx = 0):\n",
    "        super(BiRNN_encoder, self).__init__()\n",
    "        self.rnns = []\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors, freeze=False).to(device)\n",
    "        hidden_size = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        input_size = embedding_dim\n",
    "        for _ in range(num_layers):\n",
    "            if type_rnn == 'LSTM':\n",
    "                rnn = nn.LSTM(input_size, hidden_size, 1, dropout = dropout,\n",
    "                               bidirectional = bidirectional, batch_first=True).to(device)\n",
    "            elif type_rnn == 'GRU':\n",
    "                rnn = nn.GRU(input_size, hidden_size, 1, dropout = dropout,\n",
    "                              bidirectional = bidirectional, batch_first=True).to(device)\n",
    "            self.rnns.append(rnn)\n",
    "            input_size = hidden_size*2 if bidirectional else hidden_size\n",
    "        self.rnns = nn.ModuleList(self.rnns).to(device)\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.type_rnn = type_rnn\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, input, hidden = None):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        rnn_input = embedded\n",
    "        for idx, rnn in enumerate(self.rnns):\n",
    "            output, hidden_output = rnn(rnn_input, hidden)\n",
    "            hidden = hidden_output\n",
    "            rnn_input = output\n",
    "        if self.type_rnn == 'LSTM' and self.bidirectional:\n",
    "            hidden_state = torch.cat((hidden[0][-2,:,:], hidden[0][-1,:,:]), dim = 1).to(device)\n",
    "            cell = torch.cat((hidden[1][-2,:,:], hidden[1][-1,:,:]), dim = 1).to(device)\n",
    "            hidden = (hidden_state, cell)\n",
    "        elif self.type_rnn == 'GRU' and self.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1).to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\setul\\miniconda3\\envs\\talk-berty\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "encoder = BiRNN_encoder(300, 600, len(vocab), num_layers=4, type_rnn = 'LSTM',\n",
    "                         bidirectional = True, dropout = 0.3, pad_idx = vocab['<pad>']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2400])\n",
      "torch.Size([8, 2400])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(batch_dataloader):\n",
    "    context_tensor, label_tensor = batch\n",
    "    context_tensor, label_tensor = context_tensor.to(device), label_tensor.to(device)\n",
    "    hidden,cell = encoder(context_tensor)\n",
    "    print(hidden.shape)\n",
    "    print(cell.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN_decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_dim, dropout = 0.3):\n",
    "        super(BiRNN_decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        # self.embedding = nn.Embedding(vocab_dim, embedding_dim).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors, freeze=False).to(device)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout = dropout).to(device)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_dim).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, context):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        outputs, (hidden,context) = self.rnn(embedded, (hidden, context))\n",
    "        predictions = self.fc_out(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        return predictions, hidden, context\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = BiRNN_decoder(300, 2400, 1, len(vocab), dropout = 0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, hidden, teacher_ratio = 0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = len(vocab)\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        hidden, context = self.encoder(src, hidden)\n",
    "        # hidden = hidden.detach()\n",
    "        # context = context.detach()\n",
    "        dec_input = trg[:, 0]\n",
    "        dec_input = dec_input.unsqueeze(0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        context = context.unsqueeze(0)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, context = self.decoder(dec_input, hidden, context)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            dec_input = trg[:,t] if np.random.random() < teacher_ratio else top1\n",
    "            dec_input = dec_input.unsqueeze(0)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    '''\n",
    "    Evaluate the model on the given data.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    it = iter(data_loader)\n",
    "    total_count = 0. # Number of target words seen\n",
    "    total_loss = 0. # Loss over all target words\n",
    "    with torch.no_grad():\n",
    "        # No gradients need to be maintained during evaluation\n",
    "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "        hidden = None \n",
    "        for i, batch in enumerate(it):\n",
    "            text, target = batch\n",
    "            text, target = text.to(device), target.to(device)\n",
    "            output = model(text, target, hidden).to(device)\n",
    "            mask = (target != PAD_IDX)\n",
    "            ntotal = mask.sum()\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            total_count += ntotal\n",
    "            total_loss += loss.item()/ntotal\n",
    "                \n",
    "    # loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##ChatGPT's improvement\n",
    "# def evaluate(model, data_loader):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     total_count = 0  # Total number of non-<pad> tokens\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for text, target in data_loader:\n",
    "#             if USE_CUDA:\n",
    "#                 text, target = text.cuda(), target.cuda()\n",
    "\n",
    "#             output = model(text, target)\n",
    "#             mask = (target != PAD_IDX)  # Create a mask for non-pad tokens\n",
    "#             ntotal = mask.sum().item()  # Sum the mask values to get the total number of non-<pad> tokens\n",
    "\n",
    "#             # Apply mask to filter out loss contributions from <pad> tokens and compute the loss\n",
    "#             loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "#             loss = loss.masked_select(mask.view(-1)).mean()  # Only consider non-<pad> tokens and compute mean loss\n",
    "            \n",
    "#             total_loss += loss.item() * ntotal  # Accumulate the total loss\n",
    "#             total_count += ntotal  # Accumulate the total count of non-<pad> tokens\n",
    "\n",
    "#     average_loss = total_loss / total_count  # Compute the average loss over all non-<pad> tokens\n",
    "#     model.train()\n",
    "#     return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 2167.66357421875\n",
      "epoch: 0, iteration: 0\n",
      "best model, val loss:  tensor(516.3895, device='cuda:0')\n",
      "epoch 0 iter 1000 loss 1071.380615234375\n",
      "epoch 0 iter 2000 loss 611.8499145507812\n",
      "epoch 0 iter 3000 loss 857.4074096679688\n",
      "epoch 0 iter 4000 loss 933.5927734375\n",
      "epoch 0 iter 5000 loss 895.7481689453125\n",
      "epoch 0 iter 6000 loss 1260.715576171875\n",
      "epoch 0 iter 7000 loss 785.7858276367188\n",
      "epoch 0 iter 8000 loss 1048.9405517578125\n"
     ]
    }
   ],
   "source": [
    "LOG_FILE = \"language-model.log\"\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 10\n",
    "PAD_IDX = vocab['<pad>']\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if h is None:\n",
    "        return None\n",
    "    elif isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "    \n",
    "seq_model = Seq2Seq(encoder, decoder).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX, reduction = 'sum') ## Used instead of NLLLoss.\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(seq_model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "best_model = None\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    seq_model.train()\n",
    "    it = iter(batch_dataloader)\n",
    "    hidden = None\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output = seq_model(data, target, hidden).to(device)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[:,1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        mask = (target != PAD_IDX)\n",
    "        #ntotal = mask.sum()\n",
    "        loss = loss_fn(output, target)\n",
    "        # loss = loss[mask]\n",
    "        # loss = loss.mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq_model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            val_loss = evaluate(seq_model, val_dataloader)\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                print(\"epoch: {}, iteration: {}\".format(epoch, i))\n",
    "                fout.write(\"epoch: {}, iteration: {}, perplexity: {}\\n\".format(epoch, i, val_loss))\n",
    "\n",
    "            # Save the model if the validation loss is the minimum so far\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print(\"best model, val loss: \", val_loss)\n",
    "                # #best_model = copy.deepcopy(model)\n",
    "                # best_model = type(seq_model)(vocab_size, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
    "                # if USE_CUDA:\n",
    "                #     best_model = best_model.cuda()\n",
    "                # best_model.load_state_dict(model.state_dict())\n",
    "\n",
    "                # with open(PATH + \"lm-best.th\", \"wb\") as fout:\n",
    "                #     torch.save(best_model.state_dict(), fout)\n",
    "            else:\n",
    "                learning_rate /= 4.\n",
    "                optimizer = torch.optim.Adam(seq_model.parameters(), lr=learning_rate)\n",
    "            val_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##From other sources\n",
    "# epoch_loss = 0.0\n",
    "# num_epochs = 10\n",
    "# best_loss = 999999\n",
    "# best_epoch = -1\n",
    "# sentence1 = \"Hello I am starting\"\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = vocab['<pad>'])\n",
    "# ts1 = []\n",
    "# for epoch in range(num_epochs):\n",
    "#   print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "#   model.train(True)\n",
    "#   for batch_idx, batch in enumerate(batch_dataloader):\n",
    "#     input , target = batch\n",
    "#     input, target = input.to(device), target.to(device)\n",
    "#     output = model(input, target).to(device)\n",
    "#     output = output[1:].reshape(-1, output.shape[2])\n",
    "#     target = target[:,1:].reshape(-1)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss = criterion(output, target)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Clip gradient >1 to prevent exploding gradients\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "#     # Update the weights values using the gradients we calculated using bp \n",
    "#     optimizer.step()\n",
    "#     #step += 1\n",
    "#     epoch_loss += loss.item()\n",
    "#     #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "\n",
    "#   if epoch_loss < best_loss:\n",
    "#     best_loss = epoch_loss\n",
    "#     best_epoch = epoch\n",
    "#     if ((epoch - best_epoch) >= 3):\n",
    "#       print(\"no improvement in 3 epochs, break\")\n",
    "#       break\n",
    "#   print(\"Epoch_Loss - {}\".format(loss.item()))\n",
    "#   print()\n",
    "  \n",
    "# print(epoch_loss / len(batch_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq_model.state_dict(), 'seq_model_3_layers.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('seq_model_functional.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_genre, input_title, input_text, max_length=10):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and prepare input text\n",
    "    input_tokens = (tokenizer(input_genre) + ['<pad>'] + tokenizer(input_title) + ['<pad>'] +\n",
    "    ['<BOS>'] + tokenizer(input_text) + ['<EOS>'])\n",
    "    input_indices = vocab.lookup_indices(input_tokens)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, context = model.encoder(input_tensor)\n",
    "    decoder_input = torch.tensor([[vocab['<BOS>']]], device=device)  \n",
    "    output_indices = []\n",
    "    context = context.unsqueeze(0)\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, context = model.decoder(decoder_input, hidden, context)\n",
    "            output_probabilities = output.squeeze().softmax(dim = -1).to(device)\n",
    "            top1 = torch.multinomial(output_probabilities, 1)[0]\n",
    "            if top1.item() == vocab['<EOS>']:\n",
    "                break\n",
    "            output_indices.append(top1.item())\n",
    "            decoder_input = torch.tensor([[top1.item()]], device=device)  \n",
    "    output_tokens = [vocab.lookup_token(index) for index in output_indices]\n",
    "    return ' '.join(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_genre, input_title, input_text, max_length=10):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and prepare input text\n",
    "    input_tokens = (tokenizer(input_genre) + ['<pad>'] + tokenizer(input_title) + ['<pad>'] +\n",
    "    ['<BOS>'] + tokenizer(input_text) + ['<EOS>'])\n",
    "    input_indices = vocab.lookup_indices(input_tokens)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, context = model.encoder(input_tensor)\n",
    "    decoder_input = torch.tensor([[vocab['<BOS>']]], device=device)  \n",
    "    output_indices = []\n",
    "    context = context.unsqueeze(0)\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden, context = model.decoder(decoder_input, hidden, context)\n",
    "            output_probabilities = output.squeeze().softmax(dim = -1).to(device)\n",
    "            #top1 = torch.multinomial(output_probabilities, 1)[0]\n",
    "            topk_values, topk_indices = torch.topk(output_probabilities, k=10)\n",
    "            # print(topk_values)\n",
    "            # print(topk_indices)\n",
    "            top1 = torch.multinomial(topk_values, 1)[0]\n",
    "            if top1.item() == vocab['<BOS>'] or top1.item() ==vocab['<unk>'] or top1.item() ==vocab['<pad>']:\n",
    "                continue\n",
    "            if top1.item() == vocab['<EOS>'] and len(output_indices) >3:\n",
    "                break\n",
    "            elif top1.item() == vocab['<EOS>']:\n",
    "                continue\n",
    "            output_indices.append(top1.item())\n",
    "            decoder_input = torch.tensor([[top1.item()]], device=device)  \n",
    "    output_tokens = [vocab.lookup_token(index) for index in output_indices]\n",
    "    return ' '.join(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_title = \"the game\"\n",
    "input_genre = \"fiction\"\n",
    "input_sentence = \"he saw a line.\"\n",
    "generated_text = generate_text(seq_model, input_genre, input_title, input_sentence, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in seq_model.parameters():\n",
    "    print(\"param\", param.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talk-berty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
