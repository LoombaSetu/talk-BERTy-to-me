{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Change working directory to talk-berty-to-me root\n",
    "import os\n",
    "os.chdir(\"D:/University/Projects/AML/talk-berty-to-me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_gutenberg = pd.read_csv('data/books_and_genres.csv')\n",
    "# dataset_train = data_gutenberg.sample(frac=0.6, random_state=0)\n",
    "# dataset_val = data_gutenberg.drop(dataset_train.index).sample(frac=0.5, random_state=0)\n",
    "# dataset_test = data_gutenberg.drop(dataset_train.index).drop(\n",
    "#     dataset_val.index)\n",
    "# dataset_train.to_parquet('data/datasets/train.parquet', index=False)\n",
    "# dataset_test.to_parquet('data/datasets/test.parquet', index=False)\n",
    "# dataset_val.to_parquet('data/datasets/val.parquet', index=False)\n",
    "# data_gutenberg.sample(20).to_parquet('data/datasets/dev.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add code to switch between datasets here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = pd.read_parquet('data/datasets/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/books_and_genres_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>the power and the glory</td>\n",
       "      <td>Produced by Juliet Sutherland, Sjaani and PG D...</td>\n",
       "      <td>{'literary-fiction', 'christian', 'history', '...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>paradise</td>\n",
       "      <td>Produced by Judith Smith and Natalie Salter\\n\\...</td>\n",
       "      <td>{'literary-fiction', 'mythology', 'historical-...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88</td>\n",
       "      <td>sonnets</td>\n",
       "      <td>Produced by Paul Murray, Rénald Lévesque and t...</td>\n",
       "      <td>{'read-for-school', 'poetry', '20th-century', ...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "      <td>sonnets</td>\n",
       "      <td>Produced by Paul Murray, Rénald Lévesque and t...</td>\n",
       "      <td>{'read-for-school', 'poetry', '20th-century', ...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147</td>\n",
       "      <td>persuasion</td>\n",
       "      <td>Produced by Sharon Partridge and Martin Ward. ...</td>\n",
       "      <td>{'romance', 'literary-fiction', 'classics', 'h...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    title  \\\n",
       "0          31  the power and the glory   \n",
       "1          32                 paradise   \n",
       "2          88                  sonnets   \n",
       "3          88                  sonnets   \n",
       "4         147               persuasion   \n",
       "\n",
       "                                                text  \\\n",
       "0  Produced by Juliet Sutherland, Sjaani and PG D...   \n",
       "1  Produced by Judith Smith and Natalie Salter\\n\\...   \n",
       "2  Produced by Paul Murray, Rénald Lévesque and t...   \n",
       "3  Produced by Paul Murray, Rénald Lévesque and t...   \n",
       "4  Produced by Sharon Partridge and Martin Ward. ...   \n",
       "\n",
       "                                              genres language_code  \n",
       "0  {'literary-fiction', 'christian', 'history', '...           eng  \n",
       "1  {'literary-fiction', 'mythology', 'historical-...           eng  \n",
       "2  {'read-for-school', 'poetry', '20th-century', ...           eng  \n",
       "3  {'read-for-school', 'poetry', '20th-century', ...           eng  \n",
       "4  {'romance', 'literary-fiction', 'classics', 'h...           eng  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = raw_data.sample(2)\n",
    "dev_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "###Creating a validation set\n",
    "val_data = raw_data.sample(1)\n",
    "val_data.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>the time machine</td>\n",
       "      <td>The Time Machine, by H. G. Wells [1898]\\n\\n\\n\\...</td>\n",
       "      <td>{'adventure', 'novella', 'historical-fiction',...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>crime and punishment</td>\n",
       "      <td>Produced by John Bickers; and Dagny\\n\\n\\n\\n\\n\\...</td>\n",
       "      <td>{'literary-fiction', 'crime', 'drama', 'myster...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                                               text  \\\n",
       "327      the time machine  The Time Machine, by H. G. Wells [1898]\\n\\n\\n\\...   \n",
       "238  crime and punishment  Produced by John Bickers; and Dagny\\n\\n\\n\\n\\n\\...   \n",
       "\n",
       "                                                genres language_code  \n",
       "327  {'adventure', 'novella', 'historical-fiction',...           eng  \n",
       "238  {'literary-fiction', 'crime', 'drama', 'myster...           eng  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>Produced by Bruce W. Miller\\n\\n\\n\\n\\n\\n\\n\\n\\nR...</td>\n",
       "      <td>{'adventure', 'roman', 'classics', 'novels', '...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title                                               text  \\\n",
       "379  robinson crusoe  Produced by Bruce W. Miller\\n\\n\\n\\n\\n\\n\\n\\n\\nR...   \n",
       "\n",
       "                                                genres language_code  \n",
       "379  {'adventure', 'roman', 'classics', 'novels', '...           eng  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\setul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\setul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Function to clean text of books. Removes email addresses, new lines, html tags, and extra spaces.\n",
    "\n",
    "    Input: Text (String)\n",
    "    Output: Cleaned Text (String)\n",
    "    '''\n",
    "    cleaned_text = text.lower()\n",
    "    cleaned_text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' ', text)\n",
    "    cleaned_text = re.sub(r'^.*?(?=\\n\\n\\n)', ' ', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"[^>]*>.*?</a>', ' ', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s.?!]', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r' +', ' ', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_first_row(group):\n",
    "    return group.iloc[1:]\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['cleaned_text'] = dev_data['text'].apply(lambda x:clean_text(x))\n",
    "dev_data.loc[:,'genres'] = dev_data.loc[:,'genres'].apply(lambda x:clean_text(x))\n",
    "dev_data.loc[:,'sentences'] = dev_data.loc[:,'cleaned_text'].apply(lambda x: nltk.tokenize.sent_tokenize(str(x)))\n",
    "dev_data = dev_data.explode('sentences')\n",
    "dev_data.loc[:,'sentences'] = dev_data.loc[:,'sentences'].apply(lambda x:lowercase(x))\n",
    "dev_data.reset_index(drop=True, inplace=True)\n",
    "dev_data['label_sentences'] = dev_data.groupby('title')['sentences'].shift(-1)\n",
    "dev_data = dev_data.dropna(subset=['label_sentences'])\n",
    "dev_data = dev_data.loc[:,['title', 'genres','sentences', 'label_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['cleaned_text'] = val_data['text'].apply(lambda x:clean_text(x))\n",
    "val_data.loc[:,'genres'] = val_data.loc[:,'genres'].apply(lambda x:clean_text(x))\n",
    "val_data.loc[:,'sentences'] = val_data.loc[:,'cleaned_text'].apply(lambda x: nltk.tokenize.sent_tokenize(str(x)))\n",
    "val_data = val_data.explode('sentences')\n",
    "val_data.loc[:,'sentences'] = val_data.loc[:,'sentences'].apply(lambda x:lowercase(x))\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "val_data['label_sentences'] = val_data.groupby('title')['sentences'].shift(-1)\n",
    "val_data = val_data.dropna(subset=['label_sentences'])\n",
    "val_data = val_data.loc[:,['title', 'genres','sentences', 'label_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the time machine</td>\n",
       "      <td>adventure novella historical fiction science ...</td>\n",
       "      <td>i the time traveller for so it will be conven...</td>\n",
       "      <td>his grey eyes shone and twinkled and his usual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the time machine</td>\n",
       "      <td>adventure novella historical fiction science ...</td>\n",
       "      <td>his grey eyes shone and twinkled and his usual...</td>\n",
       "      <td>the fire burned brightly and the soft radiance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the time machine</td>\n",
       "      <td>adventure novella historical fiction science ...</td>\n",
       "      <td>the fire burned brightly and the soft radiance...</td>\n",
       "      <td>our chairs being his patents embraced and care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the time machine</td>\n",
       "      <td>adventure novella historical fiction science ...</td>\n",
       "      <td>our chairs being his patents embraced and care...</td>\n",
       "      <td>and he put it to us in this way marking the po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the time machine</td>\n",
       "      <td>adventure novella historical fiction science ...</td>\n",
       "      <td>and he put it to us in this way marking the po...</td>\n",
       "      <td>you must follow me carefully.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title                                             genres  \\\n",
       "0  the time machine   adventure novella historical fiction science ...   \n",
       "1  the time machine   adventure novella historical fiction science ...   \n",
       "2  the time machine   adventure novella historical fiction science ...   \n",
       "3  the time machine   adventure novella historical fiction science ...   \n",
       "4  the time machine   adventure novella historical fiction science ...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0   i the time traveller for so it will be conven...   \n",
       "1  his grey eyes shone and twinkled and his usual...   \n",
       "2  the fire burned brightly and the soft radiance...   \n",
       "3  our chairs being his patents embraced and care...   \n",
       "4  and he put it to us in this way marking the po...   \n",
       "\n",
       "                                     label_sentences  \n",
       "0  his grey eyes shone and twinkled and his usual...  \n",
       "1  the fire burned brightly and the soft radiance...  \n",
       "2  our chairs being his patents embraced and care...  \n",
       "3  and he put it to us in this way marking the po...  \n",
       "4                      you must follow me carefully.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = dev_data.loc[:,['title', 'genres','sentences', 'label_sentences']]\n",
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>robinson crusoe in words of one syllable by m...</td>\n",
       "      <td>the production of a book which is adapted to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>the production of a book which is adapted to t...</td>\n",
       "      <td>the nature of the work seems to be sufficientl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>the nature of the work seems to be sufficientl...</td>\n",
       "      <td>but although as far as the subject matter is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>but although as far as the subject matter is c...</td>\n",
       "      <td>the deep interest which de foe s story has nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>the deep interest which de foe s story has nev...</td>\n",
       "      <td>it should be stated that exceptions to the rul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                             genres  \\\n",
       "0  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "1  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "2  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "3  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "4  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0   robinson crusoe in words of one syllable by m...   \n",
       "1  the production of a book which is adapted to t...   \n",
       "2  the nature of the work seems to be sufficientl...   \n",
       "3  but although as far as the subject matter is c...   \n",
       "4  the deep interest which de foe s story has nev...   \n",
       "\n",
       "                                     label_sentences  \n",
       "0  the production of a book which is adapted to t...  \n",
       "1  the nature of the work seems to be sufficientl...  \n",
       "2  but although as far as the subject matter is c...  \n",
       "3  the deep interest which de foe s story has nev...  \n",
       "4  it should be stated that exceptions to the rul...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = val_data.loc[:,['title', 'genres','sentences', 'label_sentences']]\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'save for a subsiding stir of dust the further end of the laboratory was empty.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.loc[1900, 'sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocabulary\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab_iter = iter(dev_data.loc[:,'sentences'] + dev_data.loc[:,'title'] + dev_data.loc[:,'genres'])\n",
    "def yield_tokens(train_iter):\n",
    "    for text in train_iter:\n",
    "        if not isinstance(text, str):\n",
    "            if type(text) == list:\n",
    "                for t in text:\n",
    "                    yield tokenizer(t)\n",
    "            continue\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(vocab_iter), specials=[\"<unk>\", \"<pad>\", \"<BOS>\", \"<EOS>\"], min_freq=100)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_CACHE_DIR = '/Users/setul/mlpp23/.vector_cache'\n",
    "glove = torchtext.vocab.GloVe('6B', cache=VECTOR_CACHE_DIR)\n",
    "glove_vectors = glove.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>crime and punishment</td>\n",
       "      <td>literary fiction crime drama mystery classics...</td>\n",
       "      <td>the room was close but she had not opened the ...</td>\n",
       "      <td>from the inner rooms clouds of tobacco smoke f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>crime and punishment</td>\n",
       "      <td>literary fiction crime drama mystery classics...</td>\n",
       "      <td>from the inner rooms clouds of tobacco smoke f...</td>\n",
       "      <td>the youngest child a girl of six was asleep si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>crime and punishment</td>\n",
       "      <td>literary fiction crime drama mystery classics...</td>\n",
       "      <td>the youngest child a girl of six was asleep si...</td>\n",
       "      <td>a boy a year older stood crying and shaking in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>crime and punishment</td>\n",
       "      <td>literary fiction crime drama mystery classics...</td>\n",
       "      <td>a boy a year older stood crying and shaking in...</td>\n",
       "      <td>beside him stood a girl of nine years old tall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>crime and punishment</td>\n",
       "      <td>literary fiction crime drama mystery classics...</td>\n",
       "      <td>beside him stood a girl of nine years old tall...</td>\n",
       "      <td>her arm as thin as a stick was round her broth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>crime and punishment</td>\n",
       "      <td>literary fiction crime drama mystery classics...</td>\n",
       "      <td>her arm as thin as a stick was round her broth...</td>\n",
       "      <td>she was trying to comfort him whispering somet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                             genres  \\\n",
       "2481  crime and punishment   literary fiction crime drama mystery classics...   \n",
       "2482  crime and punishment   literary fiction crime drama mystery classics...   \n",
       "2483  crime and punishment   literary fiction crime drama mystery classics...   \n",
       "2484  crime and punishment   literary fiction crime drama mystery classics...   \n",
       "2485  crime and punishment   literary fiction crime drama mystery classics...   \n",
       "2486  crime and punishment   literary fiction crime drama mystery classics...   \n",
       "\n",
       "                                              sentences  \\\n",
       "2481  the room was close but she had not opened the ...   \n",
       "2482  from the inner rooms clouds of tobacco smoke f...   \n",
       "2483  the youngest child a girl of six was asleep si...   \n",
       "2484  a boy a year older stood crying and shaking in...   \n",
       "2485  beside him stood a girl of nine years old tall...   \n",
       "2486  her arm as thin as a stick was round her broth...   \n",
       "\n",
       "                                        label_sentences  \n",
       "2481  from the inner rooms clouds of tobacco smoke f...  \n",
       "2482  the youngest child a girl of six was asleep si...  \n",
       "2483  a boy a year older stood crying and shaking in...  \n",
       "2484  beside him stood a girl of nine years old tall...  \n",
       "2485  her arm as thin as a stick was round her broth...  \n",
       "2486  she was trying to comfort him whispering somet...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.iloc[2480:2486]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "if IS_CUDA:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_batch(batch):\n",
    "    titles, genres, sentences, label_sentences = zip(*batch)\n",
    "    #Add a separator tag between title and genre\n",
    "    context = [tokenizer(g) + ['<BOS>'] + tokenizer(t) + ['<EOS>'] +\n",
    "               ['<BOS>'] + tokenizer(s) + ['<EOS>'] for t, g,\n",
    "                s in zip(titles, genres, sentences)]\n",
    "    label_sentence = [['<BOS>'] + tokenizer(s) + ['<EOS>'] for s in label_sentences]\n",
    "    label_tensor = pad_sequence([torch.tensor(vocab.lookup_indices(t)) for t in label_sentence],\n",
    "                                    padding_value=vocab['<pad>'], batch_first=True)\n",
    "    encoder_tensor = pad_sequence([torch.tensor(vocab.lookup_indices(t)) for t in context],\n",
    "                                 padding_value=vocab['<pad>'], batch_first=True)\n",
    "    return encoder_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From HW4\n",
    "from torch.utils.data import Sampler\n",
    "class BatchSequentialSampler(Sampler):\n",
    "    r\"\"\"Samples batches, s.t. the ith elements of each batch are sequential.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        num_batches = len(self.data_source)//self.batch_size\n",
    "        for i in range(num_batches):\n",
    "            for j in range(self.batch_size):\n",
    "                yield(j * num_batches + i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_source)//self.batch_size) * self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = BatchSequentialSampler(dev_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataloader = torch.utils.data.DataLoader(dev_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']].values,\n",
    "                                                   batch_size=8, collate_fn=collate_batch, sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = BatchSequentialSampler(val_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(val_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']].values,\n",
    "                                                   batch_size=8, collate_fn=collate_batch, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 73])\n",
      "torch.Size([8, 57])\n"
     ]
    }
   ],
   "source": [
    "for idx, (context_tensor, label_tensor) in enumerate(batch_dataloader):\n",
    "    print(context_tensor.shape)\n",
    "    print(label_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN_encoder(nn.Module):\n",
    "    def __init__ (self, embedding_dim, hidden_dim,\n",
    "                  vocab_size, num_layers=2, type_rnn = 'LSTM', bidirectional = True,\n",
    "                  dropout = 0.3, pad_idx = 0):\n",
    "        super(BiRNN_encoder, self).__init__()\n",
    "        self.rnns = []\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors).to(device)\n",
    "        hidden_size = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        input_size = embedding_dim\n",
    "        for _ in range(num_layers):\n",
    "            if type_rnn == 'LSTM':\n",
    "                rnn = nn.LSTM(input_size, hidden_size, 1, dropout = dropout,\n",
    "                               bidirectional = bidirectional, batch_first=True).to(device)\n",
    "            elif type_rnn == 'GRU':\n",
    "                rnn = nn.GRU(input_size, hidden_size, 1, dropout = dropout,\n",
    "                              bidirectional = bidirectional, batch_first=True).to(device)\n",
    "            self.rnns.append(rnn)\n",
    "            input_size = hidden_size*2 if bidirectional else hidden_size\n",
    "        self.rnns = nn.ModuleList(self.rnns).to(device)\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.type_rnn = type_rnn\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, input, hidden = None):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        rnn_input = embedded\n",
    "        for idx, rnn in enumerate(self.rnns):\n",
    "            output, hidden_output = rnn(rnn_input, hidden)\n",
    "            hidden = hidden_output\n",
    "            rnn_input = output\n",
    "        if self.type_rnn == 'LSTM' and self.bidirectional:\n",
    "            hidden_state = torch.cat((hidden[0][-2,:,:], hidden[0][-1,:,:]), dim = 1).to(device)\n",
    "            cell = torch.cat((hidden[1][-2,:,:], hidden[1][-1,:,:]), dim = 1).to(device)\n",
    "            hidden = (hidden_state, cell)\n",
    "        elif self.type_rnn == 'GRU' and self.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1).to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\setul\\miniconda3\\envs\\talk-berty\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "encoder = BiRNN_encoder(300, 600, len(vocab), num_layers=2, type_rnn = 'LSTM',\n",
    "                         bidirectional = True, dropout = 0.3, pad_idx = vocab['<pad>']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2400])\n",
      "torch.Size([8, 2400])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(batch_dataloader):\n",
    "    context_tensor, label_tensor = batch\n",
    "    context_tensor, label_tensor = context_tensor.to(device), label_tensor.to(device)\n",
    "    hidden,cell = encoder(context_tensor)\n",
    "    print(hidden.shape)\n",
    "    print(cell.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN_decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_dim, dropout = 0.3):\n",
    "        super(BiRNN_decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        # self.embedding = nn.Embedding(vocab_dim, embedding_dim).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors).to(device)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout = dropout).to(device)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_dim).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, context):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        outputs, (hidden,context) = self.rnn(embedded, (hidden, context))\n",
    "        predictions = self.fc_out(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        return predictions, hidden, context\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = BiRNN_decoder(300, 2400, 1, len(vocab), dropout = 0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_ratio = 0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = len(vocab)\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        hidden, context = self.encoder(src)\n",
    "        hidden = hidden.detach()\n",
    "        context = context.detach()\n",
    "        dec_input = trg[:, 0]\n",
    "        dec_input = dec_input.unsqueeze(0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        context = context.unsqueeze(0)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, context = self.decoder(dec_input, hidden, context)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            dec_input = trg[:,t] if np.random.random() < teacher_ratio else top1\n",
    "            dec_input = dec_input.unsqueeze(0)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    '''\n",
    "    Evaluate the model on the given data.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    it = iter(data_loader)\n",
    "    total_count = 0. # Number of target words seen\n",
    "    total_loss = 0. # Loss over all target words\n",
    "    with torch.no_grad():\n",
    "        # No gradients need to be maintained during evaluation\n",
    "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "        hidden = None \n",
    "        for i, batch in enumerate(it):\n",
    "            text, target = batch\n",
    "            if USE_CUDA:\n",
    "                text, target = text.cuda(), target.cuda()\n",
    "            output = model(text, target).to(device)\n",
    "            mask = (target != PAD_IDX)\n",
    "            ntotal = mask.sum()\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            \n",
    "            total_count += np.multiply(*text.size())\n",
    "            total_loss += loss.item()*np.multiply(*text.size())\n",
    "                \n",
    "    loss = total_loss / (total_count*ntotal)\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ChatGPT's improvement\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0  # Total number of non-<pad> tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, target in data_loader:\n",
    "            if USE_CUDA:\n",
    "                text, target = text.cuda(), target.cuda()\n",
    "\n",
    "            output = model(text, target)\n",
    "            mask = (target != PAD_IDX)  # Create a mask for non-pad tokens\n",
    "            ntotal = mask.sum().item()  # Sum the mask values to get the total number of non-<pad> tokens\n",
    "\n",
    "            # Apply mask to filter out loss contributions from <pad> tokens and compute the loss\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            loss = loss.masked_select(mask.view(-1)).mean()  # Only consider non-<pad> tokens and compute mean loss\n",
    "            \n",
    "            total_loss += loss.item() * ntotal  # Accumulate the total loss\n",
    "            total_count += ntotal  # Accumulate the total count of non-<pad> tokens\n",
    "\n",
    "    average_loss = total_loss / total_count  # Compute the average loss over all non-<pad> tokens\n",
    "    model.train()\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 2.804075241088867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\setul\\AppData\\Local\\Temp\\ipykernel_6940\\4058405361.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  print(\"epoch: {}, iteration: {}, perplexity: {}\".format(epoch, i, np.exp(val_loss)))\n",
      "C:\\Users\\setul\\AppData\\Local\\Temp\\ipykernel_6940\\4058405361.py:49: RuntimeWarning: overflow encountered in exp\n",
      "  fout.write(\"epoch: {}, iteration: {}, perplexity: {}\\n\".format(epoch, i, np.exp(val_loss)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0, perplexity: inf\n",
      "best model, val loss:  801.8182156087834\n",
      "epoch 1 iter 0 loss 2.869710683822632\n",
      "epoch: 1, iteration: 0, perplexity: inf\n",
      "epoch 2 iter 0 loss 2.8594765663146973\n",
      "epoch: 2, iteration: 0, perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "##From class notes\n",
    "LOG_FILE = \"language-model.log\"\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 3\n",
    "PAD_IDX = vocab['<pad>']\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if h is None:\n",
    "        return None\n",
    "    elif isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "    \n",
    "seq_model = Seq2Seq(encoder, decoder).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX, reduction = 'sum') ## Used instead of NLLLoss.\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(seq_model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "best_model = None\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    seq_model.train()\n",
    "    it = iter(batch_dataloader)\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = seq_model(data, target).to(device)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[:,1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        mask = (target != PAD_IDX)\n",
    "        ntotal = mask.sum()\n",
    "        loss = loss_fn(output, target)\n",
    "        loss = loss / ntotal\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq_model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            val_loss = evaluate(seq_model, val_dataloader)\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                print(\"epoch: {}, iteration: {}, perplexity: {}\".format(epoch, i, np.exp(val_loss)))\n",
    "                fout.write(\"epoch: {}, iteration: {}, perplexity: {}\\n\".format(epoch, i, np.exp(val_loss)))\n",
    "\n",
    "            # Save the model if the validation loss is the minimum so far\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print(\"best model, val loss: \", val_loss)\n",
    "                # #best_model = copy.deepcopy(model)\n",
    "                # best_model = type(seq_model)(vocab_size, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
    "                # if USE_CUDA:\n",
    "                #     best_model = best_model.cuda()\n",
    "                # best_model.load_state_dict(model.state_dict())\n",
    "\n",
    "                # with open(PATH + \"lm-best.th\", \"wb\") as fout:\n",
    "                #     torch.save(best_model.state_dict(), fout)\n",
    "            else:\n",
    "                learning_rate /= 4.\n",
    "                optimizer = torch.optim.Adam(seq_model.parameters(), lr=learning_rate)\n",
    "            val_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##From other sources\n",
    "epoch_loss = 0.0\n",
    "num_epochs = 10\n",
    "best_loss = 999999\n",
    "best_epoch = -1\n",
    "sentence1 = \"Hello I am starting\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab['<pad>'])\n",
    "ts1 = []\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "  model.train(True)\n",
    "  for batch_idx, batch in enumerate(batch_dataloader):\n",
    "    input , target = batch\n",
    "    input, target = input.to(device), target.to(device)\n",
    "    output = model(input, target).to(device)\n",
    "    output = output[1:].reshape(-1, output.shape[2])\n",
    "    target = target[:,1:].reshape(-1)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradient >1 to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "    # Update the weights values using the gradients we calculated using bp \n",
    "    optimizer.step()\n",
    "    #step += 1\n",
    "    epoch_loss += loss.item()\n",
    "    #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "\n",
    "  if epoch_loss < best_loss:\n",
    "    best_loss = epoch_loss\n",
    "    best_epoch = epoch\n",
    "    if ((epoch - best_epoch) >= 3):\n",
    "      print(\"no improvement in 3 epochs, break\")\n",
    "      break\n",
    "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
    "  print()\n",
    "  \n",
    "print(epoch_loss / len(batch_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_title = \"The Adventures of Sherlock Holmes\"\n",
    "input_genre = \"Mystery\"\n",
    "input_sentence = \"I am a detective\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_genre, input_title, input_text, max_length=10):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and prepare input text\n",
    "    input_tokens = (tokenizer(input_genre) + ['<BOS>'] + tokenizer(input_title) + ['<EOS>'] +\n",
    "    ['<BOS>'] + tokenizer(input_text) + ['<EOS>'])\n",
    "    input_indices = vocab.lookup_indices(input_tokens)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, context = model.encoder(input_tensor)\n",
    "    decoder_input = torch.tensor([[vocab['<BOS>']]], device=device)  \n",
    "    output_indices = []\n",
    "    context = context.unsqueeze(0)\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            print(\"decoder_shape\", decoder_input.shape)\n",
    "            print(\"hidden_shape\", hidden.shape)\n",
    "            print(\"context_shape\", context.shape)\n",
    "            output, hidden, context = model.decoder(decoder_input, hidden, context)\n",
    "            output_probabilities = output.squeeze().exp().to(device)\n",
    "            top1 = torch.multinomial(output_probabilities, 1)[0]\n",
    "            print(top1)\n",
    "            print(\"top1 shape\", top1.shape)\n",
    "            print(vocab.lookup_token(top1.item()))\n",
    "            #print(vocab.lookup_indices(list(top1)))\n",
    "            # print(top1)\n",
    "            if top1.item() == vocab['<EOS>']:\n",
    "                break\n",
    "            output_indices.append(top1.item())\n",
    "\n",
    "            decoder_input = torch.tensor([[top1.item()]], device=device)  \n",
    "            print(\"post top1 shape\",top1.shape)\n",
    "    output_tokens = [vocab.lookup_token(index) for index in output_indices]\n",
    "    return ' '.join(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(101, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "down\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(0, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "<unk>\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(8, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "the\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(29, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "i\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(0, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "<unk>\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(10, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "and\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(10, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "and\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(41, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "was\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(20, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "time\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(71, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "one\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(52, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "but\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(0, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "<unk>\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(0, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "<unk>\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(0, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "<unk>\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(0, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "<unk>\n",
      "post top1 shape torch.Size([])\n",
      "decoder_shape torch.Size([1, 1])\n",
      "hidden_shape torch.Size([1, 1, 2400])\n",
      "context_shape torch.Size([1, 1, 2400])\n",
      "tensor(3, device='cuda:0')\n",
      "top1 shape torch.Size([])\n",
      "<EOS>\n",
      "down <unk> the i <unk> and and was time one but <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "input_title = \"autobiography\"\n",
    "input_genre = \"biography\"\n",
    "input_sentence = \"Which essay seems to you to be most successful\"\n",
    "generated_text = generate_text(seq_model, input_genre, input_title, input_sentence, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2319, -0.0882, -0.0197,  ..., -0.0957,  0.3904,  0.7836],\n",
      "        [-0.3143,  0.0605, -0.3070,  ..., -0.4419, -0.0970, -0.0613],\n",
      "        [-0.4937,  0.1265, -0.2384,  ..., -0.4252, -0.4086,  0.2398]],\n",
      "       device='cuda:0') None\n",
      "Parameter containing:\n",
      "tensor([[ 0.0169,  0.0027,  0.0187,  ...,  0.0156, -0.0032,  0.0058],\n",
      "        [-0.0067, -0.0156, -0.0198,  ...,  0.0279,  0.0008, -0.0212],\n",
      "        [ 0.0221,  0.0104,  0.0057,  ...,  0.0167, -0.0257, -0.0233],\n",
      "        ...,\n",
      "        [ 0.0001,  0.0144, -0.0146,  ...,  0.0166, -0.0168, -0.0189],\n",
      "        [-0.0047,  0.0257,  0.0095,  ..., -0.0039,  0.0161,  0.0113],\n",
      "        [-0.0115,  0.0090, -0.0170,  ...,  0.0009, -0.0017, -0.0286]],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048, -0.0083,  0.0177,  ...,  0.0236, -0.0016, -0.0243],\n",
      "        [ 0.0096,  0.0172, -0.0176,  ..., -0.0140, -0.0278, -0.0244],\n",
      "        [-0.0061, -0.0109,  0.0171,  ..., -0.0155,  0.0256,  0.0201],\n",
      "        ...,\n",
      "        [ 0.0002, -0.0176, -0.0061,  ...,  0.0131, -0.0014,  0.0028],\n",
      "        [ 0.0051,  0.0154, -0.0215,  ..., -0.0282,  0.0156, -0.0031],\n",
      "        [ 0.0139,  0.0171,  0.0192,  ...,  0.0287, -0.0242,  0.0056]],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([ 0.0243,  0.0200,  0.0004,  ...,  0.0091,  0.0213, -0.0189],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([ 0.0089, -0.0031, -0.0155,  ...,  0.0019, -0.0221, -0.0165],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[-2.2146e-02, -2.2268e-02,  1.4372e-02,  ...,  6.3833e-03,\n",
      "         -1.3175e-02,  2.7149e-02],\n",
      "        [-1.0620e-02,  1.6024e-02, -1.6977e-03,  ...,  1.8433e-02,\n",
      "          1.4602e-02, -2.5294e-02],\n",
      "        [ 9.8609e-03, -1.9997e-04, -2.4711e-02,  ..., -2.5235e-02,\n",
      "         -1.9413e-02,  8.5371e-03],\n",
      "        ...,\n",
      "        [-6.9990e-04, -1.8852e-02, -5.7094e-03,  ...,  3.4919e-03,\n",
      "          2.7126e-03, -1.5653e-02],\n",
      "        [ 2.3037e-02, -3.5383e-03,  1.2316e-02,  ...,  8.6300e-05,\n",
      "         -1.9358e-02, -1.9135e-02],\n",
      "        [-1.9922e-02, -1.1475e-02,  2.4641e-02,  ...,  6.1358e-03,\n",
      "          1.6325e-02,  1.7867e-03]], device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[ 0.0150,  0.0271, -0.0159,  ...,  0.0031,  0.0248, -0.0101],\n",
      "        [-0.0265,  0.0273, -0.0038,  ...,  0.0190,  0.0145,  0.0055],\n",
      "        [ 0.0019, -0.0053, -0.0209,  ...,  0.0172, -0.0163,  0.0247],\n",
      "        ...,\n",
      "        [ 0.0183, -0.0171, -0.0049,  ...,  0.0160, -0.0125,  0.0262],\n",
      "        [ 0.0184,  0.0057, -0.0153,  ..., -0.0172,  0.0282, -0.0054],\n",
      "        [ 0.0031,  0.0164, -0.0206,  ..., -0.0268, -0.0139, -0.0184]],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([ 0.0087,  0.0258, -0.0004,  ...,  0.0057, -0.0092,  0.0090],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([ 0.0267, -0.0156,  0.0078,  ...,  0.0043,  0.0064, -0.0031],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[ 0.0161,  0.0034,  0.0101,  ...,  0.0229,  0.0114, -0.0117],\n",
      "        [ 0.0053, -0.0203, -0.0254,  ..., -0.0269,  0.0137, -0.0245],\n",
      "        [-0.0053, -0.0288,  0.0036,  ..., -0.0214,  0.0267,  0.0202],\n",
      "        ...,\n",
      "        [-0.0112,  0.0098, -0.0004,  ...,  0.0079,  0.0146,  0.0043],\n",
      "        [ 0.0132, -0.0202, -0.0085,  ..., -0.0246, -0.0248, -0.0040],\n",
      "        [ 0.0082, -0.0145,  0.0138,  ...,  0.0084,  0.0170, -0.0073]],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[ 0.0129,  0.0055, -0.0019,  ...,  0.0003,  0.0288, -0.0179],\n",
      "        [-0.0253,  0.0045, -0.0025,  ..., -0.0125,  0.0017,  0.0113],\n",
      "        [ 0.0194, -0.0266,  0.0187,  ...,  0.0097, -0.0235,  0.0114],\n",
      "        ...,\n",
      "        [ 0.0162,  0.0152,  0.0161,  ..., -0.0260, -0.0073,  0.0224],\n",
      "        [ 0.0137, -0.0045,  0.0022,  ..., -0.0146,  0.0273,  0.0071],\n",
      "        [-0.0083,  0.0247,  0.0125,  ..., -0.0252,  0.0229,  0.0024]],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([-0.0043,  0.0071,  0.0068,  ...,  0.0158, -0.0091, -0.0169],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([ 0.0209, -0.0020, -0.0225,  ..., -0.0171,  0.0013,  0.0084],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[-0.0186, -0.0288, -0.0119,  ...,  0.0162,  0.0171,  0.0108],\n",
      "        [-0.0270, -0.0210,  0.0089,  ..., -0.0077,  0.0195,  0.0075],\n",
      "        [-0.0235,  0.0078,  0.0204,  ..., -0.0177, -0.0125,  0.0148],\n",
      "        ...,\n",
      "        [-0.0133, -0.0192, -0.0125,  ...,  0.0198,  0.0185,  0.0095],\n",
      "        [ 0.0191,  0.0005,  0.0283,  ..., -0.0002, -0.0154, -0.0175],\n",
      "        [ 0.0208, -0.0227, -0.0027,  ...,  0.0183, -0.0145,  0.0171]],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[-0.0285,  0.0206, -0.0159,  ..., -0.0259,  0.0178, -0.0272],\n",
      "        [ 0.0206, -0.0055, -0.0285,  ..., -0.0257, -0.0108, -0.0154],\n",
      "        [-0.0152, -0.0097, -0.0078,  ..., -0.0187, -0.0141, -0.0073],\n",
      "        ...,\n",
      "        [ 0.0174, -0.0132,  0.0115,  ..., -0.0243, -0.0147, -0.0177],\n",
      "        [-0.0176, -0.0018,  0.0118,  ..., -0.0116,  0.0177,  0.0248],\n",
      "        [-0.0125, -0.0085, -0.0073,  ...,  0.0134,  0.0174, -0.0130]],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([ 0.0168, -0.0016, -0.0131,  ...,  0.0093,  0.0226, -0.0072],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([-0.0131,  0.0044,  0.0271,  ..., -0.0049,  0.0217,  0.0288],\n",
      "       device='cuda:0', requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.2319, -0.0882, -0.0197,  ..., -0.0957,  0.3904,  0.7836],\n",
      "        [-0.3143,  0.0605, -0.3070,  ..., -0.4419, -0.0970, -0.0613],\n",
      "        [-0.4937,  0.1265, -0.2384,  ..., -0.4252, -0.4086,  0.2398]],\n",
      "       device='cuda:0') None\n",
      "Parameter containing:\n",
      "tensor([[ 0.0535, -0.0516, -0.0427,  ...,  0.0667,  0.0495, -0.0366],\n",
      "        [ 0.0323,  0.0049, -0.0168,  ...,  0.0509,  0.0685, -0.0027],\n",
      "        [ 0.0344, -0.0503, -0.0094,  ...,  0.0537, -0.0164, -0.0694],\n",
      "        ...,\n",
      "        [ 0.0346,  0.0742, -0.0508,  ..., -0.0085,  0.0064, -0.0302],\n",
      "        [ 0.0671,  0.0145,  0.0366,  ..., -0.0077, -0.0201, -0.0453],\n",
      "        [ 0.0701, -0.0232,  0.0017,  ...,  0.0091,  0.0092, -0.0344]],\n",
      "       device='cuda:0', requires_grad=True) tensor([[-3.2089e-08, -1.0545e-08,  3.4395e-09,  ...,  2.3514e-08,\n",
      "          2.0648e-08,  2.3225e-09],\n",
      "        [-9.2929e-08, -3.4203e-08,  4.8971e-08,  ..., -1.9496e-07,\n",
      "         -9.0599e-08,  3.1081e-07],\n",
      "        [-7.9893e-07,  3.3494e-07, -6.3616e-07,  ...,  4.0995e-08,\n",
      "          8.7034e-07, -9.5677e-08],\n",
      "        ...,\n",
      "        [-2.7242e-06,  2.5786e-06, -3.4663e-07,  ..., -1.4907e-07,\n",
      "         -4.7200e-06,  2.9702e-06],\n",
      "        [ 4.3918e-07, -2.7364e-07,  3.6238e-07,  ..., -1.2906e-07,\n",
      "         -2.2627e-07,  2.1040e-07],\n",
      "        [-1.3591e-08,  1.6857e-08, -1.8239e-08,  ..., -9.4438e-09,\n",
      "         -5.0876e-08,  1.3205e-08]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.1223,  0.0039,  0.0133,  ..., -0.1649,  0.2246, -0.2559],\n",
      "        [-0.0197,  0.0315, -0.0094,  ..., -0.0516,  0.0070, -0.0252],\n",
      "        [-0.0148,  0.0003,  0.0106,  ..., -0.0431, -0.0047, -0.0249],\n",
      "        ...,\n",
      "        [-0.0961, -0.0054,  0.0210,  ...,  0.0176,  0.0218,  0.0400],\n",
      "        [-0.0244,  0.0257, -0.0141,  ..., -0.0637,  0.0306, -0.0019],\n",
      "        [-0.0337,  0.0373,  0.0461,  ..., -0.0847,  0.0036, -0.0311]],\n",
      "       device='cuda:0', requires_grad=True) tensor([[ 9.7761e-08,  1.0035e-07,  3.7092e-09,  ..., -2.5207e-07,\n",
      "          6.8319e-08, -6.2255e-08],\n",
      "        [ 3.9817e-08,  3.9684e-08, -2.1913e-08,  ...,  3.9706e-09,\n",
      "         -1.5590e-09, -1.0289e-08],\n",
      "        [-4.0359e-08, -2.7914e-07,  1.0844e-07,  ...,  4.3204e-07,\n",
      "         -5.6731e-07,  2.4393e-07],\n",
      "        ...,\n",
      "        [ 1.8788e-07, -9.6036e-08, -5.9941e-07,  ...,  3.0568e-06,\n",
      "         -1.1759e-06,  2.8650e-07],\n",
      "        [-9.8519e-08, -1.5089e-07, -2.2911e-09,  ...,  5.7167e-07,\n",
      "         -2.9229e-07,  1.2466e-07],\n",
      "        [ 7.3511e-09, -1.2020e-09, -9.5573e-09,  ...,  5.1304e-08,\n",
      "         -2.5331e-08,  7.4230e-09]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-0.0494, -0.1090, -0.0903,  ..., -0.0306, -0.1199, -0.1561],\n",
      "       device='cuda:0', requires_grad=True) tensor([-5.7583e-06,  3.1668e-08,  2.1289e-05,  ...,  4.9138e-05,\n",
      "         1.4306e-05,  1.0327e-06], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([-0.0273, -0.1104, -0.0937,  ..., -0.0599, -0.1110, -0.1543],\n",
      "       device='cuda:0', requires_grad=True) tensor([-5.7583e-06,  3.1668e-08,  2.1289e-05,  ...,  4.9138e-05,\n",
      "         1.4306e-05,  1.0327e-06], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.1180, -0.0102, -0.0656,  ...,  0.0339, -0.0534,  0.0430],\n",
      "        [-0.1556, -0.0394,  0.0090,  ...,  0.0665, -0.0350,  0.0423],\n",
      "        [-0.1053,  0.0669,  0.0240,  ..., -0.0808,  0.1010, -0.0115],\n",
      "        ...,\n",
      "        [-0.1463,  0.0354,  0.0804,  ..., -0.0831,  0.0652, -0.0177],\n",
      "        [-0.2292,  0.0022,  0.1707,  ..., -0.0820,  0.0538, -0.0611],\n",
      "        [-0.0300,  0.0394, -0.0114,  ..., -0.1531,  0.0385, -0.1354]],\n",
      "       device='cuda:0', requires_grad=True) tensor([[ 3.9939e-04,  1.2411e-04, -2.7279e-04,  ...,  8.4721e-05,\n",
      "         -8.0432e-05,  5.5764e-06],\n",
      "        [ 5.2432e-05, -3.3859e-05, -2.8757e-05,  ...,  3.4294e-04,\n",
      "         -7.8629e-05,  6.5725e-06],\n",
      "        [ 3.5416e-10,  7.0981e-11, -2.1800e-10,  ...,  1.3134e-10,\n",
      "         -7.7761e-11,  5.0715e-12],\n",
      "        ...,\n",
      "        [ 7.1874e-07,  3.6314e-06, -1.3077e-06,  ..., -7.4476e-06,\n",
      "          1.2786e-06, -1.6775e-07],\n",
      "        [ 1.7729e-07, -1.6085e-07, -9.9704e-08,  ...,  5.1597e-07,\n",
      "         -1.2281e-07,  1.2694e-08],\n",
      "        [ 2.1897e-06,  2.6041e-07, -1.0188e-06,  ..., -3.6269e-08,\n",
      "         -3.5457e-07,  4.7736e-09]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([ 0.0805,  0.1160, -0.2239,  0.0479, -0.0798, -0.0695, -0.2066, -0.2080,\n",
      "         0.0069,  0.0494, -0.0071, -0.0843, -0.0748, -0.1088, -0.1005, -0.1521,\n",
      "        -0.1299, -0.1183, -0.1478,  0.0380, -0.0513, -0.0901, -0.0706, -0.1036,\n",
      "        -0.0720, -0.0846, -0.1915,  0.0369,  0.0517,  0.0190, -0.0773, -0.0593,\n",
      "        -0.0594, -0.0729, -0.0936, -0.1160, -0.2045, -0.2065, -0.2185,  0.0137,\n",
      "        -0.0093,  0.0141,  0.0089, -0.0037, -0.0301, -0.0149, -0.0076, -0.0248,\n",
      "         0.0176, -0.0219,  0.0182,  0.0064, -0.0009, -0.0502, -0.0146, -0.0098,\n",
      "        -0.0228,  0.0019, -0.0095, -0.0382, -0.0308, -0.0103, -0.0015, -0.0285,\n",
      "         0.0102,  0.0356, -0.0107, -0.0429, -0.0260, -0.0155, -0.0324, -0.0430,\n",
      "        -0.0092, -0.0457, -0.0085, -0.0502, -0.0400, -0.0236, -0.0395, -0.0215,\n",
      "        -0.0156, -0.0345, -0.0441, -0.0071, -0.0611, -0.0409, -0.0225, -0.0294,\n",
      "        -0.0296, -0.0537, -0.0450, -0.0289, -0.0222, -0.0245, -0.0541, -0.0644,\n",
      "        -0.0400, -0.0606, -0.0373, -0.0556, -0.0424, -0.0389, -0.0689, -0.0347,\n",
      "        -0.0299, -0.0500, -0.0556, -0.0516, -0.0574, -0.0481, -0.0444, -0.0332,\n",
      "        -0.0526, -0.0382, -0.0342, -0.0485, -0.0563, -0.0453, -0.0533, -0.0621,\n",
      "        -0.0678, -0.0286, -0.0588, -0.0450, -0.0515, -0.0774, -0.0433,  0.0071,\n",
      "        -0.0540, -0.0542, -0.0585, -0.0274, -0.0303, -0.0530, -0.0287, -0.0573,\n",
      "        -0.0547, -0.0434, -0.0691, -0.0510, -0.0673, -0.0476, -0.0542, -0.0500,\n",
      "        -0.0476, -0.0564, -0.0684, -0.0599, -0.0646, -0.0438, -0.0470, -0.0726,\n",
      "        -0.0457, -0.0077, -0.0519, -0.0745, -0.0384, -0.0330, -0.0646, -0.0375,\n",
      "        -0.0418, -0.0746, -0.0469, -0.0598, -0.0625, -0.0616, -0.0827, -0.0585,\n",
      "        -0.0508, -0.0529, -0.0686, -0.0232, -0.0479, -0.0697, -0.0734, -0.0613,\n",
      "        -0.0559, -0.0374, -0.0615, -0.0881,  0.0019, -0.0592, -0.0559, -0.0601,\n",
      "        -0.0395, -0.0368, -0.0284, -0.0784, -0.0661, -0.0584, -0.0338, -0.0762,\n",
      "        -0.0426, -0.0661, -0.0386, -0.0346, -0.0508, -0.0633, -0.0727, -0.0507,\n",
      "        -0.0694, -0.0290, -0.0487, -0.0450, -0.0589, -0.0514, -0.0621, -0.0798,\n",
      "        -0.0433, -0.0733, -0.0514, -0.0874, -0.0863, -0.0031, -0.0725, -0.0396,\n",
      "        -0.0471, -0.0605, -0.0859, -0.0774, -0.0888, -0.0522, -0.0753, -0.0407,\n",
      "        -0.0949, -0.0676, -0.0593, -0.0329, -0.0765, -0.0630, -0.0525, -0.0537,\n",
      "        -0.0831, -0.0716, -0.0765, -0.0974, -0.0769, -0.0577, -0.0199, -0.0692,\n",
      "        -0.0764, -0.0616, -0.0052, -0.0532, -0.0903, -0.0680, -0.0638, -0.0496,\n",
      "        -0.0568, -0.0537, -0.0733, -0.0410, -0.0605, -0.0580, -0.0901, -0.0667,\n",
      "        -0.0381, -0.0840, -0.0715, -0.0725, -0.0396, -0.0372, -0.0467, -0.0684,\n",
      "        -0.0195, -0.0520, -0.0996, -0.0595, -0.0740, -0.0368, -0.0831, -0.0920,\n",
      "        -0.0600, -0.0802, -0.0566, -0.0796, -0.0664, -0.0699, -0.0458, -0.0696,\n",
      "        -0.0857, -0.0625, -0.1015, -0.0853, -0.0781, -0.0697, -0.0417, -0.0592,\n",
      "        -0.0802, -0.0702, -0.0792, -0.0816, -0.0635, -0.0547, -0.0621, -0.0706,\n",
      "        -0.0755, -0.0846, -0.0793, -0.0548, -0.0776, -0.0638, -0.0495, -0.0401,\n",
      "        -0.0986, -0.0914, -0.0657, -0.0586, -0.0703, -0.1001, -0.1015, -0.0884,\n",
      "        -0.0794, -0.0837, -0.0875, -0.0616, -0.0827, -0.1079, -0.0717, -0.0696,\n",
      "        -0.0775, -0.0852, -0.0721, -0.0500, -0.0535, -0.0761, -0.0731, -0.0951,\n",
      "        -0.0768, -0.0576], device='cuda:0', requires_grad=True) tensor([-6.0379e-03,  2.7649e-02,  5.1133e-09,  2.1860e-03,  5.6932e-06,\n",
      "         1.4887e-05,  5.1835e-09,  5.2359e-09, -1.1391e-03,  2.0833e-03,\n",
      "         2.2583e-03,  8.8995e-05,  2.1220e-05,  4.6083e-06,  1.5315e-06,\n",
      "         3.8694e-07,  8.6994e-07,  5.0775e-08,  1.1307e-07,  2.7344e-03,\n",
      "         3.6685e-04,  1.3115e-04,  7.6649e-05,  6.7637e-05,  2.6479e-05,\n",
      "         2.4406e-05,  5.1942e-09,  2.0643e-03,  6.2978e-03, -9.2120e-03,\n",
      "        -2.3894e-03, -5.7472e-03, -2.9988e-03,  3.5568e-05,  9.5391e-06,\n",
      "         7.9113e-07,  5.2205e-09,  5.2185e-09,  5.1626e-09,  8.0628e-04,\n",
      "         4.7775e-03,  5.3333e-03,  4.2581e-03, -1.0138e-03,  1.5726e-03,\n",
      "        -7.9488e-03,  4.5884e-04, -2.9560e-03,  2.8494e-04,  3.7086e-03,\n",
      "         2.7922e-03, -1.1064e-04,  2.9088e-03,  1.7758e-03, -7.5223e-04,\n",
      "         2.5776e-03, -9.6375e-04,  3.7109e-03,  2.0012e-03,  1.8828e-03,\n",
      "        -4.4889e-03,  1.9259e-03, -4.3695e-03, -4.1755e-03, -9.8778e-04,\n",
      "         3.0229e-03,  1.6281e-03, -5.1445e-03,  1.3877e-03, -8.0495e-03,\n",
      "        -4.7152e-03, -1.9539e-03,  1.3493e-03, -4.9191e-03, -1.9329e-03,\n",
      "         1.0554e-03,  1.0972e-03, -1.8934e-03,  1.2463e-03,  1.4114e-03,\n",
      "         9.9154e-04, -2.0217e-03,  1.1843e-03,  8.8720e-04, -2.1573e-03,\n",
      "         9.2611e-04,  4.3835e-04, -2.2042e-03,  8.7708e-04,  7.2888e-04,\n",
      "        -2.1986e-03,  7.9204e-04,  9.1730e-04,  6.7344e-04, -2.2939e-03,\n",
      "         6.4679e-04,  8.1628e-04,  7.6200e-04,  8.3272e-04,  7.3918e-04,\n",
      "        -2.4793e-03,  7.5747e-04, -5.6041e-03,  5.4401e-04,  6.3662e-04,\n",
      "         7.0677e-04,  5.9155e-04,  5.9774e-04,  6.4495e-04,  6.8439e-04,\n",
      "         4.9028e-04,  7.5252e-04,  5.1233e-04,  4.8179e-04, -5.5924e-03,\n",
      "         5.8181e-04,  6.1796e-04,  6.8742e-04,  5.4323e-04,  4.8391e-04,\n",
      "        -2.5841e-03,  5.9899e-04,  5.0466e-04, -5.8203e-03,  5.1798e-04,\n",
      "         3.8973e-04,  4.7606e-04, -2.0445e-03,  4.9191e-04,  4.5881e-04,\n",
      "         4.8395e-04, -2.4697e-03,  4.5785e-04,  3.9818e-04,  5.5176e-04,\n",
      "         4.7242e-04,  4.6827e-04,  4.9507e-04,  4.1295e-04,  4.4491e-04,\n",
      "         4.5569e-04, -2.8464e-03,  4.1062e-04,  4.5386e-04,  4.7448e-04,\n",
      "         3.3487e-04, -2.7641e-03,  3.7293e-04,  4.2129e-04,  3.4621e-04,\n",
      "         3.5935e-04,  3.3700e-04,  4.2225e-04, -2.2858e-03,  2.8932e-04,\n",
      "         4.1544e-04,  4.5630e-04,  3.9898e-04,  4.6551e-04,  3.5596e-04,\n",
      "         4.3805e-04,  3.1778e-04,  3.8555e-04,  3.2212e-04,  2.9977e-04,\n",
      "         3.9345e-04,  3.7764e-04,  3.6886e-04,  3.7777e-04,  4.6819e-04,\n",
      "         3.1909e-04,  3.8155e-04,  2.8063e-04,  4.9192e-04,  3.1657e-04,\n",
      "        -2.8662e-03,  2.6600e-04, -2.6429e-03,  3.2660e-04, -5.9260e-03,\n",
      "         6.7713e-04, -2.8138e-03,  4.0871e-04,  3.6640e-04,  3.5086e-04,\n",
      "        -2.8099e-03,  3.4027e-04,  3.6329e-04,  1.9795e-04,  3.1228e-04,\n",
      "         3.2708e-04,  3.1759e-04,  3.2616e-04,  3.0914e-04, -2.7915e-03,\n",
      "         3.3331e-04,  3.8497e-04,  4.6215e-04,  2.9408e-04,  3.0038e-04,\n",
      "         2.8530e-04,  1.6004e-04,  3.6044e-04,  2.6866e-04,  2.7350e-04,\n",
      "         3.0824e-04,  3.3031e-04,  2.4155e-04,  3.0000e-04,  3.5973e-04,\n",
      "         2.5139e-04,  2.2432e-04,  3.1426e-04,  4.9381e-04,  2.5132e-04,\n",
      "         3.4154e-04,  2.9064e-04,  2.4730e-04,  1.8151e-04,  2.4124e-04,\n",
      "         1.7379e-04, -2.8614e-03,  3.0104e-04,  2.5692e-04,  2.2457e-04,\n",
      "         2.2652e-04,  3.3057e-04,  2.6329e-04,  2.1964e-04,  2.5244e-04,\n",
      "         2.3630e-04,  2.0349e-04,  2.2161e-04,  2.4661e-04,  2.9798e-04,\n",
      "         2.3455e-04,  1.8839e-04,  2.5360e-04,  1.3918e-04,  2.3155e-04,\n",
      "         2.8717e-04,  1.7845e-04,  3.1972e-04,  1.4630e-04, -2.9093e-03,\n",
      "         2.4075e-04,  2.1313e-04,  1.5979e-04,  2.3334e-04,  2.5563e-04,\n",
      "         2.0919e-04,  1.1518e-04,  1.6748e-04,  2.1328e-04,  1.7967e-04,\n",
      "         3.9526e-04,  2.3186e-04,  2.2891e-04,  2.4238e-04,  1.9641e-04,\n",
      "         2.1318e-04, -2.9057e-03,  1.6009e-04,  1.7231e-04,  2.3421e-04,\n",
      "         1.8348e-04,  2.0242e-04,  2.4495e-04,  1.5875e-04,  2.0478e-04,\n",
      "        -2.9641e-03,  1.4512e-04,  1.8614e-04,  1.6907e-04,  1.4414e-04,\n",
      "         1.5205e-04,  2.2998e-04,  2.0115e-04,  1.8527e-04,  1.6274e-04,\n",
      "         1.4685e-04,  1.3312e-04,  1.5907e-04,  1.2779e-04,  1.3643e-04,\n",
      "         1.8605e-04,  1.8312e-04,  1.8443e-04,  1.4095e-04,  1.3102e-04,\n",
      "         2.3512e-04,  1.6772e-04,  1.5835e-04,  1.1889e-04,  8.2882e-05,\n",
      "         1.5072e-04,  2.0003e-04,  1.8808e-04,  1.4001e-04,  1.4857e-04,\n",
      "         1.6884e-04, -2.9638e-03,  1.3062e-04,  1.2712e-04,  1.2729e-04,\n",
      "         1.2022e-04,  1.0573e-04, -2.9559e-03, -2.9538e-03,  8.9700e-05,\n",
      "         1.4663e-04,  1.2224e-04,  1.2324e-04,  1.5627e-04,  1.7234e-04,\n",
      "         1.5926e-04,  1.7413e-04,  1.2694e-04,  1.4350e-04,  1.2807e-04,\n",
      "         1.0278e-04,  1.5220e-04,  1.3975e-04,  1.2209e-04,  1.2319e-04,\n",
      "         1.4665e-04,  1.0559e-04, -2.9935e-03,  1.2578e-04,  1.2512e-04],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for param in seq_model.parameters():\n",
    "    print(\"param\", param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talk-berty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
