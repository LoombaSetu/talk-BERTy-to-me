{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Change working directory to talk-berty-to-me root\n",
    "import os\n",
    "os.chdir(\"D:/University/Projects/AML/talk-berty-to-me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_gutenberg = pd.read_csv('data/books_and_genres.csv')\n",
    "# dataset_train = data_gutenberg.sample(frac=0.6, random_state=0)\n",
    "# dataset_val = data_gutenberg.drop(dataset_train.index).sample(frac=0.5, random_state=0)\n",
    "# dataset_test = data_gutenberg.drop(dataset_train.index).drop(\n",
    "#     dataset_val.index)\n",
    "# dataset_train.to_parquet('data/datasets/train.parquet', index=False)\n",
    "# dataset_test.to_parquet('data/datasets/test.parquet', index=False)\n",
    "# dataset_val.to_parquet('data/datasets/val.parquet', index=False)\n",
    "# data_gutenberg.sample(20).to_parquet('data/datasets/dev.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add code to switch between datasets here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_data = pd.read_parquet('data/datasets/dev.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/books_and_genres_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>the power and the glory</td>\n",
       "      <td>Produced by Juliet Sutherland, Sjaani and PG D...</td>\n",
       "      <td>{'literary-fiction', 'christian', 'history', '...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>paradise</td>\n",
       "      <td>Produced by Judith Smith and Natalie Salter\\n\\...</td>\n",
       "      <td>{'literary-fiction', 'mythology', 'historical-...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88</td>\n",
       "      <td>sonnets</td>\n",
       "      <td>Produced by Paul Murray, Rénald Lévesque and t...</td>\n",
       "      <td>{'read-for-school', 'poetry', '20th-century', ...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "      <td>sonnets</td>\n",
       "      <td>Produced by Paul Murray, Rénald Lévesque and t...</td>\n",
       "      <td>{'read-for-school', 'poetry', '20th-century', ...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147</td>\n",
       "      <td>persuasion</td>\n",
       "      <td>Produced by Sharon Partridge and Martin Ward. ...</td>\n",
       "      <td>{'romance', 'literary-fiction', 'classics', 'h...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    title  \\\n",
       "0          31  the power and the glory   \n",
       "1          32                 paradise   \n",
       "2          88                  sonnets   \n",
       "3          88                  sonnets   \n",
       "4         147               persuasion   \n",
       "\n",
       "                                                text  \\\n",
       "0  Produced by Juliet Sutherland, Sjaani and PG D...   \n",
       "1  Produced by Judith Smith and Natalie Salter\\n\\...   \n",
       "2  Produced by Paul Murray, Rénald Lévesque and t...   \n",
       "3  Produced by Paul Murray, Rénald Lévesque and t...   \n",
       "4  Produced by Sharon Partridge and Martin Ward. ...   \n",
       "\n",
       "                                              genres language_code  \n",
       "0  {'literary-fiction', 'christian', 'history', '...           eng  \n",
       "1  {'literary-fiction', 'mythology', 'historical-...           eng  \n",
       "2  {'read-for-school', 'poetry', '20th-century', ...           eng  \n",
       "3  {'read-for-school', 'poetry', '20th-century', ...           eng  \n",
       "4  {'romance', 'literary-fiction', 'classics', 'h...           eng  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = raw_data.sample(20)\n",
    "dev_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "###Creating a validation set\n",
    "val_data = raw_data.sample(5)\n",
    "val_data.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>phaedrus</td>\n",
       "      <td>Produced by Sue Asscher\\n\\n\\n\\n\\n\\nPHAEDRUS\\n\\...</td>\n",
       "      <td>{'history', 'classics', 'mythology', 'philosop...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>Produced by David Clarke, Chuck Greif and the ...</td>\n",
       "      <td>{'literary-fiction', 'historical-fiction', 'hi...</td>\n",
       "      <td>en-US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>don quixote</td>\n",
       "      <td>Produced by David Widger\\n\\n\\n\\n\\n\\nDON QUIXOT...</td>\n",
       "      <td>{'adventure', 'literary-fiction', 'history', '...</td>\n",
       "      <td>en-US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>the jungle</td>\n",
       "      <td>Produced by David Meltzer, Christy Phillips, S...</td>\n",
       "      <td>{'literary-fiction', 'history', 'historical-fi...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>alice in wonderland</td>\n",
       "      <td>Produced by Chuck Greif, Jason Isbell and The ...</td>\n",
       "      <td>{'adventure', 'middle-grade', 'classics', 'fan...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  \\\n",
       "143                    phaedrus   \n",
       "158  the picture of dorian gray   \n",
       "405                 don quixote   \n",
       "81                   the jungle   \n",
       "179         alice in wonderland   \n",
       "\n",
       "                                                  text  \\\n",
       "143  Produced by Sue Asscher\\n\\n\\n\\n\\n\\nPHAEDRUS\\n\\...   \n",
       "158  Produced by David Clarke, Chuck Greif and the ...   \n",
       "405  Produced by David Widger\\n\\n\\n\\n\\n\\nDON QUIXOT...   \n",
       "81   Produced by David Meltzer, Christy Phillips, S...   \n",
       "179  Produced by Chuck Greif, Jason Isbell and The ...   \n",
       "\n",
       "                                                genres language_code  \n",
       "143  {'history', 'classics', 'mythology', 'philosop...           eng  \n",
       "158  {'literary-fiction', 'historical-fiction', 'hi...         en-US  \n",
       "405  {'adventure', 'literary-fiction', 'history', '...         en-US  \n",
       "81   {'literary-fiction', 'history', 'historical-fi...           eng  \n",
       "179  {'adventure', 'middle-grade', 'classics', 'fan...           eng  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>Produced by Bruce W. Miller\\n\\n\\n\\n\\n\\n\\n\\n\\nR...</td>\n",
       "      <td>{'adventure', 'roman', 'classics', 'novels', '...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>frankenstein</td>\n",
       "      <td>Produced by Judith Boss, Christy Phillips, Lyn...</td>\n",
       "      <td>{'horror', 'mystery', 'classics', 'supernatura...</td>\n",
       "      <td>en-US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>macbeth</td>\n",
       "      <td>Produced by Delphine Lettau\\n\\n\\n\\n\\nThis book...</td>\n",
       "      <td>{'history', 'historical-fiction', 'historical'...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>oedipus the king</td>\n",
       "      <td>Produced by Christos Alexandridis\\n\\n\\n\\n\\n\\nT...</td>\n",
       "      <td>{'history', 'historical', 'classics', 'high-sc...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>the girl with the golden eyes</td>\n",
       "      <td>Produced by John Bickers, and Dagny\\n\\n\\n\\n\\n ...</td>\n",
       "      <td>{'romance', 'literary-fiction', 'drama', 'roma...</td>\n",
       "      <td>eng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "378                robinson crusoe   \n",
       "385                   frankenstein   \n",
       "76                         macbeth   \n",
       "169               oedipus the king   \n",
       "145  the girl with the golden eyes   \n",
       "\n",
       "                                                  text  \\\n",
       "378  Produced by Bruce W. Miller\\n\\n\\n\\n\\n\\n\\n\\n\\nR...   \n",
       "385  Produced by Judith Boss, Christy Phillips, Lyn...   \n",
       "76   Produced by Delphine Lettau\\n\\n\\n\\n\\nThis book...   \n",
       "169  Produced by Christos Alexandridis\\n\\n\\n\\n\\n\\nT...   \n",
       "145  Produced by John Bickers, and Dagny\\n\\n\\n\\n\\n ...   \n",
       "\n",
       "                                                genres language_code  \n",
       "378  {'adventure', 'roman', 'classics', 'novels', '...           eng  \n",
       "385  {'horror', 'mystery', 'classics', 'supernatura...         en-US  \n",
       "76   {'history', 'historical-fiction', 'historical'...           eng  \n",
       "169  {'history', 'historical', 'classics', 'high-sc...           eng  \n",
       "145  {'romance', 'literary-fiction', 'drama', 'roma...           eng  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\setul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\setul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Function to clean text of books. Removes email addresses, new lines, html tags, and extra spaces.\n",
    "\n",
    "    Input: Text (String)\n",
    "    Output: Cleaned Text (String)\n",
    "    '''\n",
    "    cleaned_text = text.lower()\n",
    "    cleaned_text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', ' ', text)\n",
    "    cleaned_text = re.sub(r'^.*?(?=\\n\\n\\n)', ' ', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"[^>]*>.*?</a>', ' ', cleaned_text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'\\n', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s.?!]', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub(r' +', ' ', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_first_row(group):\n",
    "    return group.iloc[1:]\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['cleaned_text'] = dev_data['text'].apply(lambda x:clean_text(x))\n",
    "dev_data.loc[:,'genres'] = dev_data.loc[:,'genres'].apply(lambda x:clean_text(x))\n",
    "dev_data.loc[:,'sentences'] = dev_data.loc[:,'cleaned_text'].apply(lambda x: nltk.tokenize.sent_tokenize(str(x)))\n",
    "dev_data = dev_data.explode('sentences')\n",
    "dev_data.loc[:,'sentences'] = dev_data.loc[:,'sentences'].apply(lambda x:lowercase(x))\n",
    "dev_data.reset_index(drop=True, inplace=True)\n",
    "dev_data['label_sentences'] = dev_data.groupby('title')['sentences'].shift(-1)\n",
    "dev_data = dev_data.dropna(subset=['label_sentences'])\n",
    "dev_data = dev_data.loc[:,['title', 'genres','sentences', 'label_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['cleaned_text'] = val_data['text'].apply(lambda x:clean_text(x))\n",
    "val_data.loc[:,'genres'] = val_data.loc[:,'genres'].apply(lambda x:clean_text(x))\n",
    "val_data.loc[:,'sentences'] = val_data.loc[:,'cleaned_text'].apply(lambda x: nltk.tokenize.sent_tokenize(str(x)))\n",
    "val_data = val_data.explode('sentences')\n",
    "val_data.loc[:,'sentences'] = val_data.loc[:,'sentences'].apply(lambda x:lowercase(x))\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "val_data['label_sentences'] = val_data.groupby('title')['sentences'].shift(-1)\n",
    "val_data = val_data.dropna(subset=['label_sentences'])\n",
    "val_data = val_data.loc[:,['title', 'genres','sentences', 'label_sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phaedrus</td>\n",
       "      <td>history classics mythology philosophy fiction...</td>\n",
       "      <td>phaedrus by plato translated by benjamin jowe...</td>\n",
       "      <td>the phaedrus is closely connected with the sym...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phaedrus</td>\n",
       "      <td>history classics mythology philosophy fiction...</td>\n",
       "      <td>the phaedrus is closely connected with the sym...</td>\n",
       "      <td>the two dialogues together contain the whole p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phaedrus</td>\n",
       "      <td>history classics mythology philosophy fiction...</td>\n",
       "      <td>the two dialogues together contain the whole p...</td>\n",
       "      <td>but in the phaedrus and symposium love and phi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phaedrus</td>\n",
       "      <td>history classics mythology philosophy fiction...</td>\n",
       "      <td>but in the phaedrus and symposium love and phi...</td>\n",
       "      <td>the spiritual and emotional part is elevated i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phaedrus</td>\n",
       "      <td>history classics mythology philosophy fiction...</td>\n",
       "      <td>the spiritual and emotional part is elevated i...</td>\n",
       "      <td>whether the subject of the dialogue is love or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      title                                             genres  \\\n",
       "0  phaedrus   history classics mythology philosophy fiction...   \n",
       "1  phaedrus   history classics mythology philosophy fiction...   \n",
       "2  phaedrus   history classics mythology philosophy fiction...   \n",
       "3  phaedrus   history classics mythology philosophy fiction...   \n",
       "4  phaedrus   history classics mythology philosophy fiction...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0   phaedrus by plato translated by benjamin jowe...   \n",
       "1  the phaedrus is closely connected with the sym...   \n",
       "2  the two dialogues together contain the whole p...   \n",
       "3  but in the phaedrus and symposium love and phi...   \n",
       "4  the spiritual and emotional part is elevated i...   \n",
       "\n",
       "                                     label_sentences  \n",
       "0  the phaedrus is closely connected with the sym...  \n",
       "1  the two dialogues together contain the whole p...  \n",
       "2  but in the phaedrus and symposium love and phi...  \n",
       "3  the spiritual and emotional part is elevated i...  \n",
       "4  whether the subject of the dialogue is love or...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = dev_data.loc[:,['title', 'genres','sentences', 'label_sentences']]\n",
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>robinson crusoe in words of one syllable by m...</td>\n",
       "      <td>the production of a book which is adapted to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>the production of a book which is adapted to t...</td>\n",
       "      <td>the nature of the work seems to be sufficientl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>the nature of the work seems to be sufficientl...</td>\n",
       "      <td>but although as far as the subject matter is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>but although as far as the subject matter is c...</td>\n",
       "      <td>the deep interest which de foe s story has nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>robinson crusoe</td>\n",
       "      <td>adventure roman classics novels fiction adult...</td>\n",
       "      <td>the deep interest which de foe s story has nev...</td>\n",
       "      <td>it should be stated that exceptions to the rul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                             genres  \\\n",
       "0  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "1  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "2  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "3  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "4  robinson crusoe   adventure roman classics novels fiction adult...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0   robinson crusoe in words of one syllable by m...   \n",
       "1  the production of a book which is adapted to t...   \n",
       "2  the nature of the work seems to be sufficientl...   \n",
       "3  but although as far as the subject matter is c...   \n",
       "4  the deep interest which de foe s story has nev...   \n",
       "\n",
       "                                     label_sentences  \n",
       "0  the production of a book which is adapted to t...  \n",
       "1  the nature of the work seems to be sufficientl...  \n",
       "2  but although as far as the subject matter is c...  \n",
       "3  the deep interest which de foe s story has nev...  \n",
       "4  it should be stated that exceptions to the rul...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = val_data.loc[:,['title', 'genres','sentences', 'label_sentences']]\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i don t know which to follow.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.loc[1905, 'sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building vocabulary\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab_iter = iter(dev_data.loc[:,'sentences'] + dev_data.loc[:,'title'] + dev_data.loc[:,'genres'])\n",
    "def yield_tokens(train_iter):\n",
    "    for text in train_iter:\n",
    "        if not isinstance(text, str):\n",
    "            if type(text) == list:\n",
    "                for t in text:\n",
    "                    yield tokenizer(t)\n",
    "            continue\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(vocab_iter), specials=[\"<unk>\", \"<pad>\", \"<BOS>\", \"<EOS>\"], min_freq=50)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3284"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_CACHE_DIR = '/Users/setul/mlpp23/.vector_cache'\n",
    "glove = torchtext.vocab.GloVe('6B', cache=VECTOR_CACHE_DIR)\n",
    "glove_vectors = glove.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentences</th>\n",
       "      <th>label_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>literary fiction historical fiction historica...</td>\n",
       "      <td>he exclaimed but at that moment the duke of be...</td>\n",
       "      <td>he is gone murmured sibyl sadly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>literary fiction historical fiction historica...</td>\n",
       "      <td>he is gone murmured sibyl sadly.</td>\n",
       "      <td>i wish you had seen him.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>literary fiction historical fiction historica...</td>\n",
       "      <td>i wish you had seen him.</td>\n",
       "      <td>i wish i had for as sure as there is a god in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>literary fiction historical fiction historica...</td>\n",
       "      <td>i wish i had for as sure as there is a god in ...</td>\n",
       "      <td>she looked at him in horror.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>literary fiction historical fiction historica...</td>\n",
       "      <td>she looked at him in horror.</td>\n",
       "      <td>he repeated his words.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>the picture of dorian gray</td>\n",
       "      <td>literary fiction historical fiction historica...</td>\n",
       "      <td>he repeated his words.</td>\n",
       "      <td>they cut the air like a dagger.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title  \\\n",
       "2481  the picture of dorian gray   \n",
       "2482  the picture of dorian gray   \n",
       "2483  the picture of dorian gray   \n",
       "2484  the picture of dorian gray   \n",
       "2485  the picture of dorian gray   \n",
       "2486  the picture of dorian gray   \n",
       "\n",
       "                                                 genres  \\\n",
       "2481   literary fiction historical fiction historica...   \n",
       "2482   literary fiction historical fiction historica...   \n",
       "2483   literary fiction historical fiction historica...   \n",
       "2484   literary fiction historical fiction historica...   \n",
       "2485   literary fiction historical fiction historica...   \n",
       "2486   literary fiction historical fiction historica...   \n",
       "\n",
       "                                              sentences  \\\n",
       "2481  he exclaimed but at that moment the duke of be...   \n",
       "2482                   he is gone murmured sibyl sadly.   \n",
       "2483                           i wish you had seen him.   \n",
       "2484  i wish i had for as sure as there is a god in ...   \n",
       "2485                       she looked at him in horror.   \n",
       "2486                             he repeated his words.   \n",
       "\n",
       "                                        label_sentences  \n",
       "2481                   he is gone murmured sibyl sadly.  \n",
       "2482                           i wish you had seen him.  \n",
       "2483  i wish i had for as sure as there is a god in ...  \n",
       "2484                       she looked at him in horror.  \n",
       "2485                             he repeated his words.  \n",
       "2486                    they cut the air like a dagger.  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.iloc[2480:2486]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "if IS_CUDA:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_batch(batch):\n",
    "    titles, genres, sentences, label_sentences = zip(*batch)\n",
    "    #Add a separator tag between title and genre\n",
    "    context = [tokenizer(g) + ['<pad>'] + tokenizer(t) + ['<pad>'] +\n",
    "               ['<BOS>'] + tokenizer(s) + ['<EOS>'] for t, g,\n",
    "                s in zip(titles, genres, sentences)]\n",
    "    label_sentence = [['<BOS>'] + tokenizer(s) + ['<EOS>'] for s in label_sentences]\n",
    "    label_tensor = pad_sequence([torch.tensor(vocab.lookup_indices(t)) for t in label_sentence],\n",
    "                                    padding_value=vocab['<pad>'], batch_first=True)\n",
    "    encoder_tensor = pad_sequence([torch.tensor(vocab.lookup_indices(t)) for t in context],\n",
    "                                 padding_value=vocab['<pad>'], batch_first=True)\n",
    "    return encoder_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From HW4\n",
    "from torch.utils.data import Sampler\n",
    "class BatchSequentialSampler(Sampler):\n",
    "    r\"\"\"Samples batches, s.t. the ith elements of each batch are sequential.\n",
    "\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        num_batches = len(self.data_source)//self.batch_size\n",
    "        for i in range(num_batches):\n",
    "            for j in range(self.batch_size):\n",
    "                yield(j * num_batches + i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_source)//self.batch_size) * self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = BatchSequentialSampler(dev_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']], 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataloader = torch.utils.data.DataLoader(dev_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']].values,\n",
    "                                                   batch_size=64, collate_fn=collate_batch, sampler=batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = BatchSequentialSampler(val_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']], 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(val_data.loc[:,['title', 'genres', 'sentences', 'label_sentences']].values,\n",
    "                                                   batch_size=64, collate_fn=collate_batch, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 111])\n",
      "torch.Size([64, 88])\n"
     ]
    }
   ],
   "source": [
    "for idx, (context_tensor, label_tensor) in enumerate(batch_dataloader):\n",
    "    print(context_tensor.shape)\n",
    "    print(label_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN_encoder(nn.Module):\n",
    "    def __init__ (self, embedding_dim, hidden_dim,\n",
    "                  vocab_size, num_layers=2, type_rnn = 'LSTM', bidirectional = True,\n",
    "                  dropout = 0.3, pad_idx = 0):\n",
    "        super(BiRNN_encoder, self).__init__()\n",
    "        self.rnns = []\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors).to(device)\n",
    "        hidden_size = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        input_size = embedding_dim\n",
    "        for _ in range(num_layers):\n",
    "            if type_rnn == 'LSTM':\n",
    "                rnn = nn.LSTM(input_size, hidden_size, 1, dropout = dropout,\n",
    "                               bidirectional = bidirectional, batch_first=True).to(device)\n",
    "            elif type_rnn == 'GRU':\n",
    "                rnn = nn.GRU(input_size, hidden_size, 1, dropout = dropout,\n",
    "                              bidirectional = bidirectional, batch_first=True).to(device)\n",
    "            self.rnns.append(rnn)\n",
    "            input_size = hidden_size*2 if bidirectional else hidden_size\n",
    "        self.rnns = nn.ModuleList(self.rnns).to(device)\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        self.type_rnn = type_rnn\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, input, hidden = None):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        rnn_input = embedded\n",
    "        for idx, rnn in enumerate(self.rnns):\n",
    "            output, hidden_output = rnn(rnn_input, hidden)\n",
    "            hidden = hidden_output\n",
    "            rnn_input = output\n",
    "        if self.type_rnn == 'LSTM' and self.bidirectional:\n",
    "            hidden_state = torch.cat((hidden[0][-2,:,:], hidden[0][-1,:,:]), dim = 1).to(device)\n",
    "            cell = torch.cat((hidden[1][-2,:,:], hidden[1][-1,:,:]), dim = 1).to(device)\n",
    "            hidden = (hidden_state, cell)\n",
    "        elif self.type_rnn == 'GRU' and self.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1).to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\setul\\miniconda3\\envs\\talk-berty\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "encoder = BiRNN_encoder(300, 600, len(vocab), num_layers=2, type_rnn = 'LSTM',\n",
    "                         bidirectional = True, dropout = 0.3, pad_idx = vocab['<pad>']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2400])\n",
      "torch.Size([64, 2400])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(batch_dataloader):\n",
    "    context_tensor, label_tensor = batch\n",
    "    context_tensor, label_tensor = context_tensor.to(device), label_tensor.to(device)\n",
    "    hidden,cell = encoder(context_tensor)\n",
    "    print(hidden.shape)\n",
    "    print(cell.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN_decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_dim, dropout = 0.3):\n",
    "        super(BiRNN_decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.dropout = nn.Dropout(dropout).to(device)\n",
    "        # self.embedding = nn.Embedding(vocab_dim, embedding_dim).to(device)\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_vectors).to(device)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout = dropout).to(device)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_dim).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden, context):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        outputs, (hidden,context) = self.rnn(embedded, (hidden, context))\n",
    "        predictions = self.fc_out(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        return predictions, hidden, context\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = BiRNN_decoder(300, 2400, 1, len(vocab), dropout = 0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_ratio = 0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = len(vocab)\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        hidden, context = self.encoder(src)\n",
    "        hidden = hidden.detach()\n",
    "        context = context.detach()\n",
    "        dec_input = trg[:, 0]\n",
    "        dec_input = dec_input.unsqueeze(0)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        context = context.unsqueeze(0)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, context = self.decoder(dec_input, hidden, context)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            dec_input = trg[:,t] if np.random.random() < teacher_ratio else top1\n",
    "            dec_input = dec_input.unsqueeze(0)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    '''\n",
    "    Evaluate the model on the given data.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    it = iter(data_loader)\n",
    "    total_count = 0. # Number of target words seen\n",
    "    total_loss = 0. # Loss over all target words\n",
    "    with torch.no_grad():\n",
    "        # No gradients need to be maintained during evaluation\n",
    "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
    "        hidden = None \n",
    "        for i, batch in enumerate(it):\n",
    "            text, target = batch\n",
    "            if USE_CUDA:\n",
    "                text, target = text.cuda(), target.cuda()\n",
    "            output = model(text, target).to(device)\n",
    "            mask = (target != PAD_IDX)\n",
    "            ntotal = mask.sum()\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            \n",
    "            total_count += ntotal\n",
    "            total_loss += loss.item()*ntotal\n",
    "                \n",
    "    loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ChatGPT's improvement\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0  # Total number of non-<pad> tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, target in data_loader:\n",
    "            if USE_CUDA:\n",
    "                text, target = text.cuda(), target.cuda()\n",
    "\n",
    "            output = model(text, target)\n",
    "            mask = (target != PAD_IDX)  # Create a mask for non-pad tokens\n",
    "            ntotal = mask.sum().item()  # Sum the mask values to get the total number of non-<pad> tokens\n",
    "\n",
    "            # Apply mask to filter out loss contributions from <pad> tokens and compute the loss\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            loss = loss.masked_select(mask.view(-1)).mean()  # Only consider non-<pad> tokens and compute mean loss\n",
    "            \n",
    "            total_loss += loss.item() * ntotal  # Accumulate the total loss\n",
    "            total_count += ntotal  # Accumulate the total count of non-<pad> tokens\n",
    "\n",
    "    average_loss = total_loss / total_count  # Compute the average loss over all non-<pad> tokens\n",
    "    model.train()\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 8.098148345947266\n"
     ]
    }
   ],
   "source": [
    "LOG_FILE = \"language-model.log\"\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 10\n",
    "PAD_IDX = vocab['<pad>']\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if h is None:\n",
    "        return None\n",
    "    elif isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "    \n",
    "seq_model = Seq2Seq(encoder, decoder).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX, reduction = 'sum') ## Used instead of NLLLoss.\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(seq_model.parameters(), lr=learning_rate)\n",
    "val_losses = []\n",
    "best_model = None\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    seq_model.train()\n",
    "    it = iter(batch_dataloader)\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = seq_model(data, target).to(device)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[:,1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        mask = (target != PAD_IDX)\n",
    "        ntotal = mask.sum()\n",
    "        loss = loss_fn(output, target)\n",
    "        loss = loss / ntotal\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq_model.parameters(), GRAD_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            val_loss = evaluate(seq_model, val_dataloader)\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                #print(\"epoch: {}, iteration: {}, perplexity: {}\".format(epoch, i, np.exp(val_loss)))\n",
    "                fout.write(\"epoch: {}, iteration: {}, perplexity: {}\\n\".format(epoch, i, val_loss))\n",
    "\n",
    "            # Save the model if the validation loss is the minimum so far\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print(\"best model, val loss: \", val_loss)\n",
    "                # #best_model = copy.deepcopy(model)\n",
    "                # best_model = type(seq_model)(vocab_size, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
    "                # if USE_CUDA:\n",
    "                #     best_model = best_model.cuda()\n",
    "                # best_model.load_state_dict(model.state_dict())\n",
    "\n",
    "                # with open(PATH + \"lm-best.th\", \"wb\") as fout:\n",
    "                #     torch.save(best_model.state_dict(), fout)\n",
    "            else:\n",
    "                learning_rate /= 4.\n",
    "                optimizer = torch.optim.Adam(seq_model.parameters(), lr=learning_rate)\n",
    "            val_losses.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##From other sources\n",
    "# epoch_loss = 0.0\n",
    "# num_epochs = 10\n",
    "# best_loss = 999999\n",
    "# best_epoch = -1\n",
    "# sentence1 = \"Hello I am starting\"\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index = vocab['<pad>'])\n",
    "# ts1 = []\n",
    "# for epoch in range(num_epochs):\n",
    "#   print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
    "#   model.train(True)\n",
    "#   for batch_idx, batch in enumerate(batch_dataloader):\n",
    "#     input , target = batch\n",
    "#     input, target = input.to(device), target.to(device)\n",
    "#     output = model(input, target).to(device)\n",
    "#     output = output[1:].reshape(-1, output.shape[2])\n",
    "#     target = target[:,1:].reshape(-1)\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss = criterion(output, target)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Clip gradient >1 to prevent exploding gradients\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "#     # Update the weights values using the gradients we calculated using bp \n",
    "#     optimizer.step()\n",
    "#     #step += 1\n",
    "#     epoch_loss += loss.item()\n",
    "#     #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "\n",
    "#   if epoch_loss < best_loss:\n",
    "#     best_loss = epoch_loss\n",
    "#     best_epoch = epoch\n",
    "#     if ((epoch - best_epoch) >= 3):\n",
    "#       print(\"no improvement in 3 epochs, break\")\n",
    "#       break\n",
    "#   print(\"Epoch_Loss - {}\".format(loss.item()))\n",
    "#   print()\n",
    "  \n",
    "# print(epoch_loss / len(batch_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq_model.state_dict(), 'seq_model_functional.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_genre, input_title, input_text, max_length=10):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and prepare input text\n",
    "    input_tokens = (tokenizer(input_genre) + ['<pad>'] + tokenizer(input_title) + ['<pad>'] +\n",
    "    ['<BOS>'] + tokenizer(input_text) + ['<EOS>'])\n",
    "    input_indices = vocab.lookup_indices(input_tokens)\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, context = model.encoder(input_tensor)\n",
    "    decoder_input = torch.tensor([[vocab['<BOS>']]], device=device)  \n",
    "    output_indices = []\n",
    "    context = context.unsqueeze(0)\n",
    "    hidden = hidden.unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # print(\"decoder_shape\", decoder_input.shape)\n",
    "            # print(\"hidden_shape\", hidden.shape)\n",
    "            # print(\"context_shape\", context.shape)\n",
    "            output, hidden, context = model.decoder(decoder_input, hidden, context)\n",
    "            output_probabilities = output.squeeze().exp().to(device)\n",
    "            top1 = torch.multinomial(output_probabilities, 1)[0]\n",
    "            #top1 = torch.argmax(output)\n",
    "            # print(top1)\n",
    "            # print(\"top1 shape\", top1.shape)\n",
    "            # print(vocab.lookup_token(top1.item()))\n",
    "            #print(vocab.lookup_indices(list(top1)))\n",
    "            # print(top1)\n",
    "            if top1.item() == vocab['<EOS>']:\n",
    "                break\n",
    "            output_indices.append(top1.item())\n",
    "\n",
    "            decoder_input = torch.tensor([[top1.item()]], device=device)  \n",
    "    output_tokens = [vocab.lookup_token(index) for index in output_indices]\n",
    "    return ' '.join(output_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.lookup_token(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_title = \"autobiography\"\n",
    "input_genre = \"crime\"\n",
    "input_sentence = \"The room was close.\"\n",
    "generated_text = generate_text(seq_model, input_genre, input_title, input_sentence, max_length=50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in seq_model.parameters():\n",
    "    print(\"param\", param.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talk-berty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
